{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a348872e-f0dc-431d-8bdd-db4b534e9f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G:\\PhD\\TCANet\\jupyter-notebook-TCANet\n"
     ]
    }
   ],
   "source": [
    "cd G:\\PhD\\TCANet\\jupyter-notebook-TCANet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f04c528-da9d-4112-a0f8-1a74ece51547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 16, 22, 1000]           2,016\n",
      "            Conv2d-2          [-1, 16, 1, 1000]             368\n",
      "       BatchNorm2d-3          [-1, 16, 1, 1000]              32\n",
      "               ELU-4          [-1, 16, 1, 1000]               0\n",
      "         AvgPool2d-5            [-1, 16, 1, 17]               0\n",
      "           Dropout-6            [-1, 16, 1, 17]               0\n",
      "            Conv2d-7         [-1, 16, 22, 1000]           1,008\n",
      "            Conv2d-8          [-1, 16, 1, 1000]             368\n",
      "       BatchNorm2d-9          [-1, 16, 1, 1000]              32\n",
      "              ELU-10          [-1, 16, 1, 1000]               0\n",
      "        AvgPool2d-11            [-1, 16, 1, 17]               0\n",
      "          Dropout-12            [-1, 16, 1, 17]               0\n",
      "           Conv2d-13         [-1, 16, 22, 1000]             512\n",
      "           Conv2d-14          [-1, 16, 1, 1000]             368\n",
      "      BatchNorm2d-15          [-1, 16, 1, 1000]              32\n",
      "              ELU-16          [-1, 16, 1, 1000]               0\n",
      "        AvgPool2d-17            [-1, 16, 1, 17]               0\n",
      "          Dropout-18            [-1, 16, 1, 17]               0\n",
      "        Rearrange-19               [-1, 17, 48]               0\n",
      "           MSCNet-20               [-1, 17, 48]               0\n",
      "           Conv1d-21               [-1, 16, 17]             768\n",
      "     CausalConv1d-22               [-1, 16, 17]           3,072\n",
      "      BatchNorm1d-23               [-1, 16, 17]              32\n",
      "              ELU-24               [-1, 16, 17]               0\n",
      "              ELU-25               [-1, 16, 17]               0\n",
      "              ELU-26               [-1, 16, 17]               0\n",
      "          Dropout-27               [-1, 16, 17]               0\n",
      "     CausalConv1d-28               [-1, 16, 17]           1,024\n",
      "      BatchNorm1d-29               [-1, 16, 17]              32\n",
      "              ELU-30               [-1, 16, 17]               0\n",
      "              ELU-31               [-1, 16, 17]               0\n",
      "              ELU-32               [-1, 16, 17]               0\n",
      "          Dropout-33               [-1, 16, 17]               0\n",
      "              ELU-34               [-1, 16, 17]               0\n",
      "              ELU-35               [-1, 16, 17]               0\n",
      "              ELU-36               [-1, 16, 17]               0\n",
      "     CausalConv1d-37               [-1, 16, 17]           1,024\n",
      "      BatchNorm1d-38               [-1, 16, 17]              32\n",
      "              ELU-39               [-1, 16, 17]               0\n",
      "              ELU-40               [-1, 16, 17]               0\n",
      "              ELU-41               [-1, 16, 17]               0\n",
      "          Dropout-42               [-1, 16, 17]               0\n",
      "     CausalConv1d-43               [-1, 16, 17]           1,024\n",
      "      BatchNorm1d-44               [-1, 16, 17]              32\n",
      "              ELU-45               [-1, 16, 17]               0\n",
      "              ELU-46               [-1, 16, 17]               0\n",
      "              ELU-47               [-1, 16, 17]               0\n",
      "          Dropout-48               [-1, 16, 17]               0\n",
      "              ELU-49               [-1, 16, 17]               0\n",
      "              ELU-50               [-1, 16, 17]               0\n",
      "              ELU-51               [-1, 16, 17]               0\n",
      "        _TCNBlock-52               [-1, 17, 16]               0\n",
      "           Linear-53               [-1, 17, 16]             272\n",
      "           Linear-54               [-1, 17, 16]             272\n",
      "           Linear-55               [-1, 17, 16]             272\n",
      "          Dropout-56            [-1, 2, 17, 17]               0\n",
      "           Linear-57               [-1, 17, 16]             272\n",
      "MultiHeadAttention-58               [-1, 17, 16]               0\n",
      "          Dropout-59               [-1, 17, 16]               0\n",
      "        LayerNorm-60               [-1, 17, 16]              32\n",
      "      ResidualAdd-61               [-1, 17, 16]               0\n",
      "           Linear-62               [-1, 17, 16]             272\n",
      "           Linear-63               [-1, 17, 16]             272\n",
      "           Linear-64               [-1, 17, 16]             272\n",
      "          Dropout-65            [-1, 2, 17, 17]               0\n",
      "           Linear-66               [-1, 17, 16]             272\n",
      "MultiHeadAttention-67               [-1, 17, 16]               0\n",
      "          Dropout-68               [-1, 17, 16]               0\n",
      "        LayerNorm-69               [-1, 17, 16]              32\n",
      "      ResidualAdd-70               [-1, 17, 16]               0\n",
      "           Linear-71               [-1, 17, 16]             272\n",
      "           Linear-72               [-1, 17, 16]             272\n",
      "           Linear-73               [-1, 17, 16]             272\n",
      "          Dropout-74            [-1, 2, 17, 17]               0\n",
      "           Linear-75               [-1, 17, 16]             272\n",
      "MultiHeadAttention-76               [-1, 17, 16]               0\n",
      "          Dropout-77               [-1, 17, 16]               0\n",
      "        LayerNorm-78               [-1, 17, 16]              32\n",
      "      ResidualAdd-79               [-1, 17, 16]               0\n",
      "           Linear-80               [-1, 17, 16]             272\n",
      "           Linear-81               [-1, 17, 16]             272\n",
      "           Linear-82               [-1, 17, 16]             272\n",
      "          Dropout-83            [-1, 2, 17, 17]               0\n",
      "           Linear-84               [-1, 17, 16]             272\n",
      "MultiHeadAttention-85               [-1, 17, 16]               0\n",
      "          Dropout-86               [-1, 17, 16]               0\n",
      "        LayerNorm-87               [-1, 17, 16]              32\n",
      "      ResidualAdd-88               [-1, 17, 16]               0\n",
      "           Linear-89               [-1, 17, 16]             272\n",
      "           Linear-90               [-1, 17, 16]             272\n",
      "           Linear-91               [-1, 17, 16]             272\n",
      "          Dropout-92            [-1, 2, 17, 17]               0\n",
      "           Linear-93               [-1, 17, 16]             272\n",
      "MultiHeadAttention-94               [-1, 17, 16]               0\n",
      "          Dropout-95               [-1, 17, 16]               0\n",
      "        LayerNorm-96               [-1, 17, 16]              32\n",
      "      ResidualAdd-97               [-1, 17, 16]               0\n",
      "           Linear-98               [-1, 17, 16]             272\n",
      "           Linear-99               [-1, 17, 16]             272\n",
      "          Linear-100               [-1, 17, 16]             272\n",
      "         Dropout-101            [-1, 2, 17, 17]               0\n",
      "          Linear-102               [-1, 17, 16]             272\n",
      "MultiHeadAttention-103               [-1, 17, 16]               0\n",
      "         Dropout-104               [-1, 17, 16]               0\n",
      "       LayerNorm-105               [-1, 17, 16]              32\n",
      "     ResidualAdd-106               [-1, 17, 16]               0\n",
      "         Dropout-107               [-1, 17, 16]               0\n",
      "         Flatten-108                  [-1, 272]               0\n",
      "          Linear-109                    [-1, 4]           1,092\n",
      "          TCANet-110       [[-1, 4], [-1, 272]]               0\n",
      "  EEGTransformer-111       [[-1, 272], [-1, 4]]               0\n",
      "================================================================\n",
      "Total params: 19,588\n",
      "Trainable params: 19,588\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.08\n",
      "Forward/backward pass size (MB): 9.36\n",
      "Params size (MB): 0.07\n",
      "Estimated Total Size (MB): 9.52\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Thu Feb  5 13:03:40 2026\n",
      "seed is 432\n",
      "Subject 1\n",
      "-------------------- raw train size： (288, 1, 22, 1000) test size： (288, 22, 1000) subject: 1 fold: 1\n",
      "-------------------- train size： (232, 1, 22, 1000) val size： (56, 1, 22, 1000)\n",
      "1_0 train_acc: 0.2045 train_loss: 1.873368\tval_acc: 0.285714 val_loss: 1.4512959 test_acc:0.267361\n",
      "1_2 train_acc: 0.3864 train_loss: 1.419261\tval_acc: 0.339286 val_loss: 1.3166639 test_acc:0.256944\n",
      "1_3 train_acc: 0.2614 train_loss: 1.553358\tval_acc: 0.410714 val_loss: 1.2634732 test_acc:0.315972\n",
      "1_4 train_acc: 0.3864 train_loss: 1.461707\tval_acc: 0.410714 val_loss: 1.2492262 test_acc:0.322917\n",
      "1_5 train_acc: 0.3750 train_loss: 1.463654\tval_acc: 0.500000 val_loss: 1.1717211 test_acc:0.347222\n",
      "1_8 train_acc: 0.3977 train_loss: 1.465214\tval_acc: 0.535714 val_loss: 1.0573487 test_acc:0.392361\n",
      "1_9 train_acc: 0.4659 train_loss: 1.069394\tval_acc: 0.589286 val_loss: 1.0278641 test_acc:0.402778\n",
      "1_10 train_acc: 0.3409 train_loss: 1.409151\tval_acc: 0.589286 val_loss: 1.0091629 test_acc:0.409722\n",
      "1_12 train_acc: 0.5227 train_loss: 1.052796\tval_acc: 0.625000 val_loss: 0.9802917 test_acc:0.427083\n",
      "1_14 train_acc: 0.5114 train_loss: 1.050818\tval_acc: 0.625000 val_loss: 0.9004003 test_acc:0.468750\n",
      "1_15 train_acc: 0.5341 train_loss: 1.153520\tval_acc: 0.678571 val_loss: 0.8279424 test_acc:0.534722\n",
      "1_16 train_acc: 0.6364 train_loss: 0.936038\tval_acc: 0.732143 val_loss: 0.7809330 test_acc:0.534722\n",
      "1_17 train_acc: 0.5000 train_loss: 1.083071\tval_acc: 0.732143 val_loss: 0.6999907 test_acc:0.579861\n",
      "1_19 train_acc: 0.6364 train_loss: 0.841960\tval_acc: 0.785714 val_loss: 0.5821290 test_acc:0.572917\n",
      "1_21 train_acc: 0.6136 train_loss: 0.802533\tval_acc: 0.803571 val_loss: 0.5297891 test_acc:0.607639\n",
      "1_22 train_acc: 0.6136 train_loss: 0.810754\tval_acc: 0.803571 val_loss: 0.5031210 test_acc:0.625000\n",
      "1_26 train_acc: 0.7045 train_loss: 0.757405\tval_acc: 0.821429 val_loss: 0.5240730 test_acc:0.607639\n",
      "1_28 train_acc: 0.7500 train_loss: 0.629360\tval_acc: 0.821429 val_loss: 0.4688855 test_acc:0.625000\n",
      "1_29 train_acc: 0.6932 train_loss: 0.664081\tval_acc: 0.821429 val_loss: 0.4500085 test_acc:0.635417\n",
      "1_32 train_acc: 0.7500 train_loss: 0.565767\tval_acc: 0.875000 val_loss: 0.4408064 test_acc:0.645833\n",
      "1_44 train_acc: 0.7500 train_loss: 0.477302\tval_acc: 0.875000 val_loss: 0.3719723 test_acc:0.697917\n",
      "1_47 train_acc: 0.8750 train_loss: 0.390577\tval_acc: 0.875000 val_loss: 0.3441845 test_acc:0.666667\n",
      "1_54 train_acc: 0.8295 train_loss: 0.423977\tval_acc: 0.875000 val_loss: 0.3145564 test_acc:0.701389\n",
      "1_55 train_acc: 0.8523 train_loss: 0.407090\tval_acc: 0.875000 val_loss: 0.2940538 test_acc:0.673611\n",
      "1_60 train_acc: 0.7955 train_loss: 0.402221\tval_acc: 0.892857 val_loss: 0.2659634 test_acc:0.690972\n",
      "1_64 train_acc: 0.8295 train_loss: 0.365842\tval_acc: 0.910714 val_loss: 0.2736497 test_acc:0.718750\n",
      "1_73 train_acc: 0.8750 train_loss: 0.325662\tval_acc: 0.910714 val_loss: 0.2545417 test_acc:0.715278\n",
      "1_78 train_acc: 0.8182 train_loss: 0.369409\tval_acc: 0.910714 val_loss: 0.2502011 test_acc:0.708333\n",
      "1_83 train_acc: 0.8864 train_loss: 0.352411\tval_acc: 0.910714 val_loss: 0.2458793 test_acc:0.722222\n",
      "1_84 train_acc: 0.9318 train_loss: 0.206540\tval_acc: 0.928571 val_loss: 0.2307135 test_acc:0.739583\n",
      "1_142 train_acc: 0.9091 train_loss: 0.184763\tval_acc: 0.946429 val_loss: 0.1571387 test_acc:0.798611\n",
      "1_156 train_acc: 0.9659 train_loss: 0.120465\tval_acc: 0.946429 val_loss: 0.1068443 test_acc:0.812500\n",
      "1_169 train_acc: 0.9091 train_loss: 0.206764\tval_acc: 0.964286 val_loss: 0.1165854 test_acc:0.812500\n",
      "1_188 train_acc: 0.9318 train_loss: 0.187345\tval_acc: 0.964286 val_loss: 0.0952768 test_acc:0.829861\n",
      "1_231 train_acc: 0.9659 train_loss: 0.162224\tval_acc: 0.964286 val_loss: 0.0699796 test_acc:0.833333\n",
      "1_307 train_acc: 0.9773 train_loss: 0.044355\tval_acc: 0.964286 val_loss: 0.0689508 test_acc:0.847222\n",
      "1_320 train_acc: 0.9432 train_loss: 0.118528\tval_acc: 0.964286 val_loss: 0.0679813 test_acc:0.868056\n",
      "1_337 train_acc: 0.9886 train_loss: 0.035858\tval_acc: 0.982143 val_loss: 0.0936591 test_acc:0.864583\n",
      "1_348 train_acc: 0.9886 train_loss: 0.040141\tval_acc: 0.982143 val_loss: 0.0565803 test_acc:0.850694\n",
      "1_369 train_acc: 0.9886 train_loss: 0.068457\tval_acc: 0.982143 val_loss: 0.0525345 test_acc:0.857639\n",
      "1_374 train_acc: 0.9659 train_loss: 0.053511\tval_acc: 1.000000 val_loss: 0.0448329 test_acc:0.864583\n",
      "1_414 train_acc: 0.9545 train_loss: 0.094115\tval_acc: 1.000000 val_loss: 0.0379139 test_acc:0.840278\n",
      "1_415 train_acc: 0.9545 train_loss: 0.106657\tval_acc: 1.000000 val_loss: 0.0309311 test_acc:0.857639\n",
      "1_429 train_acc: 0.9886 train_loss: 0.033853\tval_acc: 1.000000 val_loss: 0.0259742 test_acc:0.850694\n",
      "1_436 train_acc: 0.9886 train_loss: 0.024658\tval_acc: 1.000000 val_loss: 0.0237656 test_acc:0.857639\n",
      "1_491 train_acc: 0.9773 train_loss: 0.072054\tval_acc: 1.000000 val_loss: 0.0226357 test_acc:0.861111\n",
      "1_497 train_acc: 1.0000 train_loss: 0.008461\tval_acc: 1.000000 val_loss: 0.0187636 test_acc:0.868056\n",
      "1_519 train_acc: 0.9773 train_loss: 0.078907\tval_acc: 1.000000 val_loss: 0.0172638 test_acc:0.861111\n",
      "1_626 train_acc: 0.9773 train_loss: 0.064532\tval_acc: 1.000000 val_loss: 0.0153506 test_acc:0.850694\n",
      "1_744 train_acc: 0.9773 train_loss: 0.040148\tval_acc: 1.000000 val_loss: 0.0147137 test_acc:0.864583\n",
      "1_745 train_acc: 1.0000 train_loss: 0.012168\tval_acc: 1.000000 val_loss: 0.0127266 test_acc:0.871528\n",
      "1_774 train_acc: 0.9659 train_loss: 0.041467\tval_acc: 1.000000 val_loss: 0.0125459 test_acc:0.868056\n",
      "1_777 train_acc: 1.0000 train_loss: 0.010237\tval_acc: 1.000000 val_loss: 0.0101542 test_acc:0.875000\n",
      "1_778 train_acc: 0.9886 train_loss: 0.040356\tval_acc: 1.000000 val_loss: 0.0100274 test_acc:0.881944\n",
      "1_841 train_acc: 0.9659 train_loss: 0.112601\tval_acc: 1.000000 val_loss: 0.0056400 test_acc:0.888889\n",
      "1_847 train_acc: 0.9773 train_loss: 0.051626\tval_acc: 1.000000 val_loss: 0.0049561 test_acc:0.868056\n",
      "1_848 train_acc: 0.9773 train_loss: 0.041768\tval_acc: 1.000000 val_loss: 0.0028394 test_acc:0.871528\n",
      "epoch:  848 \tThe test accuracy is: 0.8715277777777778\n",
      " THE BEST ACCURACY IS 0.8715277777777778\tkappa is 0.8287037037037037\n",
      "subject 1 duration: 0:38:06.639685\n",
      "1 subject 1 fold mean: \n",
      " accuray      87.152778\n",
      "precision    87.181944\n",
      "recall       87.152778\n",
      "f1           87.146795\n",
      "kappa        82.870370\n",
      "dtype: float64\n",
      "seed is 1352\n",
      "Subject 2\n",
      "-------------------- raw train size： (288, 1, 22, 1000) test size： (288, 22, 1000) subject: 2 fold: 1\n",
      "-------------------- train size： (232, 1, 22, 1000) val size： (56, 1, 22, 1000)\n",
      "2_0 train_acc: 0.2386 train_loss: 1.931602\tval_acc: 0.285714 val_loss: 1.3660147 test_acc:0.246528\n",
      "2_1 train_acc: 0.2614 train_loss: 1.699245\tval_acc: 0.321429 val_loss: 1.3311180 test_acc:0.291667\n",
      "2_2 train_acc: 0.3182 train_loss: 1.605948\tval_acc: 0.357143 val_loss: 1.2939012 test_acc:0.336806\n",
      "2_3 train_acc: 0.3068 train_loss: 1.497189\tval_acc: 0.428571 val_loss: 1.2423956 test_acc:0.347222\n",
      "2_4 train_acc: 0.4091 train_loss: 1.361668\tval_acc: 0.428571 val_loss: 1.2106451 test_acc:0.361111\n",
      "2_5 train_acc: 0.3636 train_loss: 1.370422\tval_acc: 0.517857 val_loss: 1.1794611 test_acc:0.385417\n",
      "2_6 train_acc: 0.3864 train_loss: 1.259934\tval_acc: 0.553571 val_loss: 1.1298283 test_acc:0.361111\n",
      "2_7 train_acc: 0.4318 train_loss: 1.408464\tval_acc: 0.589286 val_loss: 1.0760947 test_acc:0.371528\n",
      "2_9 train_acc: 0.4432 train_loss: 1.178593\tval_acc: 0.625000 val_loss: 0.9979300 test_acc:0.406250\n",
      "2_11 train_acc: 0.5455 train_loss: 1.161094\tval_acc: 0.625000 val_loss: 0.9385238 test_acc:0.406250\n",
      "2_12 train_acc: 0.4545 train_loss: 1.180263\tval_acc: 0.625000 val_loss: 0.9121776 test_acc:0.420139\n",
      "2_13 train_acc: 0.4432 train_loss: 1.132745\tval_acc: 0.678571 val_loss: 0.8845134 test_acc:0.434028\n",
      "2_18 train_acc: 0.5000 train_loss: 1.075729\tval_acc: 0.678571 val_loss: 0.7781440 test_acc:0.468750\n",
      "2_19 train_acc: 0.5568 train_loss: 1.050979\tval_acc: 0.678571 val_loss: 0.7371896 test_acc:0.472222\n",
      "2_20 train_acc: 0.5568 train_loss: 1.012694\tval_acc: 0.714286 val_loss: 0.7266580 test_acc:0.503472\n",
      "2_21 train_acc: 0.5909 train_loss: 1.008192\tval_acc: 0.732143 val_loss: 0.7175255 test_acc:0.496528\n",
      "2_22 train_acc: 0.6250 train_loss: 0.943150\tval_acc: 0.785714 val_loss: 0.6970840 test_acc:0.489583\n",
      "2_90 train_acc: 0.7386 train_loss: 0.655364\tval_acc: 0.785714 val_loss: 0.5419432 test_acc:0.517361\n",
      "2_95 train_acc: 0.8409 train_loss: 0.442597\tval_acc: 0.785714 val_loss: 0.5020600 test_acc:0.548611\n",
      "2_99 train_acc: 0.7955 train_loss: 0.490369\tval_acc: 0.803571 val_loss: 0.4873669 test_acc:0.517361\n",
      "2_104 train_acc: 0.8068 train_loss: 0.488398\tval_acc: 0.803571 val_loss: 0.4491992 test_acc:0.527778\n",
      "2_105 train_acc: 0.7386 train_loss: 0.662203\tval_acc: 0.821429 val_loss: 0.4397946 test_acc:0.524306\n",
      "2_113 train_acc: 0.8295 train_loss: 0.399365\tval_acc: 0.839286 val_loss: 0.4268546 test_acc:0.548611\n",
      "2_116 train_acc: 0.8295 train_loss: 0.439961\tval_acc: 0.857143 val_loss: 0.4188285 test_acc:0.538194\n",
      "2_129 train_acc: 0.8068 train_loss: 0.511000\tval_acc: 0.857143 val_loss: 0.3817452 test_acc:0.572917\n",
      "2_130 train_acc: 0.8182 train_loss: 0.516795\tval_acc: 0.875000 val_loss: 0.3677832 test_acc:0.569444\n",
      "2_163 train_acc: 0.8636 train_loss: 0.380202\tval_acc: 0.875000 val_loss: 0.3404059 test_acc:0.538194\n",
      "2_165 train_acc: 0.8523 train_loss: 0.437811\tval_acc: 0.892857 val_loss: 0.3419948 test_acc:0.527778\n",
      "2_175 train_acc: 0.8977 train_loss: 0.369820\tval_acc: 0.928571 val_loss: 0.2913473 test_acc:0.559028\n",
      "2_183 train_acc: 0.8636 train_loss: 0.423270\tval_acc: 0.928571 val_loss: 0.2763641 test_acc:0.548611\n",
      "2_213 train_acc: 0.8864 train_loss: 0.343579\tval_acc: 0.928571 val_loss: 0.2695236 test_acc:0.559028\n",
      "2_239 train_acc: 0.9432 train_loss: 0.239287\tval_acc: 0.928571 val_loss: 0.2556857 test_acc:0.569444\n",
      "2_240 train_acc: 0.8636 train_loss: 0.436850\tval_acc: 0.928571 val_loss: 0.2412522 test_acc:0.545139\n",
      "2_246 train_acc: 0.8750 train_loss: 0.408899\tval_acc: 0.928571 val_loss: 0.2317169 test_acc:0.541667\n",
      "2_247 train_acc: 0.9205 train_loss: 0.272800\tval_acc: 0.928571 val_loss: 0.1965687 test_acc:0.559028\n",
      "2_254 train_acc: 0.9091 train_loss: 0.217314\tval_acc: 0.946429 val_loss: 0.2378145 test_acc:0.576389\n",
      "2_272 train_acc: 0.9318 train_loss: 0.209550\tval_acc: 0.946429 val_loss: 0.1790913 test_acc:0.572917\n",
      "2_282 train_acc: 0.8977 train_loss: 0.251432\tval_acc: 0.946429 val_loss: 0.1569225 test_acc:0.562500\n",
      "2_284 train_acc: 0.9545 train_loss: 0.173582\tval_acc: 0.946429 val_loss: 0.1477976 test_acc:0.545139\n",
      "2_286 train_acc: 0.9205 train_loss: 0.209686\tval_acc: 0.964286 val_loss: 0.1418179 test_acc:0.583333\n",
      "2_297 train_acc: 0.9545 train_loss: 0.195122\tval_acc: 0.982143 val_loss: 0.1323816 test_acc:0.548611\n",
      "2_298 train_acc: 0.9659 train_loss: 0.162906\tval_acc: 0.982143 val_loss: 0.1315472 test_acc:0.555556\n",
      "2_345 train_acc: 0.9205 train_loss: 0.263870\tval_acc: 0.982143 val_loss: 0.1080083 test_acc:0.590278\n",
      "2_358 train_acc: 0.9318 train_loss: 0.212562\tval_acc: 0.982143 val_loss: 0.1073430 test_acc:0.562500\n",
      "2_387 train_acc: 0.9318 train_loss: 0.223847\tval_acc: 0.982143 val_loss: 0.0934307 test_acc:0.600694\n",
      "2_412 train_acc: 0.9091 train_loss: 0.145491\tval_acc: 0.982143 val_loss: 0.0785226 test_acc:0.618056\n",
      "2_422 train_acc: 0.9432 train_loss: 0.146088\tval_acc: 1.000000 val_loss: 0.0689133 test_acc:0.586806\n",
      "2_455 train_acc: 0.9091 train_loss: 0.272967\tval_acc: 1.000000 val_loss: 0.0679004 test_acc:0.611111\n",
      "2_486 train_acc: 0.9205 train_loss: 0.167504\tval_acc: 1.000000 val_loss: 0.0572864 test_acc:0.649306\n",
      "2_533 train_acc: 0.9205 train_loss: 0.200412\tval_acc: 1.000000 val_loss: 0.0395173 test_acc:0.645833\n",
      "2_534 train_acc: 0.9545 train_loss: 0.119445\tval_acc: 1.000000 val_loss: 0.0356633 test_acc:0.621528\n",
      "2_561 train_acc: 0.9432 train_loss: 0.153802\tval_acc: 1.000000 val_loss: 0.0336485 test_acc:0.635417\n",
      "2_729 train_acc: 0.9886 train_loss: 0.036074\tval_acc: 1.000000 val_loss: 0.0310295 test_acc:0.649306\n",
      "2_730 train_acc: 0.9773 train_loss: 0.046064\tval_acc: 1.000000 val_loss: 0.0297083 test_acc:0.645833\n",
      "2_732 train_acc: 0.9659 train_loss: 0.094296\tval_acc: 1.000000 val_loss: 0.0258242 test_acc:0.631944\n",
      "2_733 train_acc: 0.9659 train_loss: 0.072979\tval_acc: 1.000000 val_loss: 0.0233024 test_acc:0.607639\n",
      "2_736 train_acc: 0.9545 train_loss: 0.187669\tval_acc: 1.000000 val_loss: 0.0174107 test_acc:0.614583\n",
      "2_828 train_acc: 0.9886 train_loss: 0.030292\tval_acc: 1.000000 val_loss: 0.0143860 test_acc:0.673611\n",
      "2_862 train_acc: 0.9886 train_loss: 0.027812\tval_acc: 1.000000 val_loss: 0.0136712 test_acc:0.663194\n",
      "epoch:  862 \tThe test accuracy is: 0.6631944444444444\n",
      " THE BEST ACCURACY IS 0.6631944444444444\tkappa is 0.5509259259259259\n",
      "subject 2 duration: 0:49:17.412228\n",
      "2 subject 1 fold mean: \n",
      " accuray      66.319444\n",
      "precision    71.223812\n",
      "recall       66.319444\n",
      "f1           65.415177\n",
      "kappa        55.092593\n",
      "dtype: float64\n",
      "seed is 1332\n",
      "Subject 3\n",
      "-------------------- raw train size： (288, 1, 22, 1000) test size： (288, 22, 1000) subject: 3 fold: 1\n",
      "-------------------- train size： (232, 1, 22, 1000) val size： (56, 1, 22, 1000)\n",
      "3_0 train_acc: 0.3068 train_loss: 1.917078\tval_acc: 0.267857 val_loss: 1.4731277 test_acc:0.270833\n",
      "3_2 train_acc: 0.3636 train_loss: 1.529379\tval_acc: 0.285714 val_loss: 1.3972483 test_acc:0.354167\n",
      "3_3 train_acc: 0.4318 train_loss: 1.373694\tval_acc: 0.321429 val_loss: 1.3530810 test_acc:0.395833\n",
      "3_4 train_acc: 0.4205 train_loss: 1.310325\tval_acc: 0.339286 val_loss: 1.2917330 test_acc:0.402778\n",
      "3_6 train_acc: 0.3523 train_loss: 1.393216\tval_acc: 0.464286 val_loss: 1.1937968 test_acc:0.465278\n",
      "3_7 train_acc: 0.4205 train_loss: 1.347900\tval_acc: 0.464286 val_loss: 1.1304240 test_acc:0.517361\n",
      "3_8 train_acc: 0.3864 train_loss: 1.340101\tval_acc: 0.500000 val_loss: 1.1064085 test_acc:0.517361\n",
      "3_9 train_acc: 0.5114 train_loss: 1.136261\tval_acc: 0.571429 val_loss: 1.0592215 test_acc:0.545139\n",
      "3_11 train_acc: 0.4773 train_loss: 1.136255\tval_acc: 0.589286 val_loss: 0.9918223 test_acc:0.548611\n",
      "3_12 train_acc: 0.5227 train_loss: 1.069034\tval_acc: 0.607143 val_loss: 0.9500228 test_acc:0.559028\n",
      "3_13 train_acc: 0.5000 train_loss: 1.133869\tval_acc: 0.625000 val_loss: 0.9303227 test_acc:0.552083\n",
      "3_15 train_acc: 0.6364 train_loss: 0.866790\tval_acc: 0.642857 val_loss: 0.8680840 test_acc:0.572917\n",
      "3_16 train_acc: 0.5568 train_loss: 1.066413\tval_acc: 0.678571 val_loss: 0.8282444 test_acc:0.586806\n",
      "3_20 train_acc: 0.5568 train_loss: 1.039214\tval_acc: 0.678571 val_loss: 0.7391864 test_acc:0.642361\n",
      "3_21 train_acc: 0.5455 train_loss: 0.899605\tval_acc: 0.678571 val_loss: 0.7236077 test_acc:0.631944\n",
      "3_22 train_acc: 0.7386 train_loss: 0.718056\tval_acc: 0.696429 val_loss: 0.6901725 test_acc:0.673611\n",
      "3_25 train_acc: 0.6932 train_loss: 0.731671\tval_acc: 0.696429 val_loss: 0.6879951 test_acc:0.670139\n",
      "3_26 train_acc: 0.6364 train_loss: 0.873815\tval_acc: 0.696429 val_loss: 0.6115789 test_acc:0.687500\n",
      "3_27 train_acc: 0.7614 train_loss: 0.569494\tval_acc: 0.696429 val_loss: 0.5665316 test_acc:0.687500\n",
      "3_28 train_acc: 0.7614 train_loss: 0.585266\tval_acc: 0.696429 val_loss: 0.5624257 test_acc:0.690972\n",
      "3_29 train_acc: 0.7500 train_loss: 0.710369\tval_acc: 0.767857 val_loss: 0.4730200 test_acc:0.715278\n",
      "3_32 train_acc: 0.7273 train_loss: 0.589077\tval_acc: 0.803571 val_loss: 0.5342536 test_acc:0.736111\n",
      "3_34 train_acc: 0.7273 train_loss: 0.677310\tval_acc: 0.803571 val_loss: 0.4942913 test_acc:0.763889\n",
      "3_35 train_acc: 0.7841 train_loss: 0.531377\tval_acc: 0.839286 val_loss: 0.4104106 test_acc:0.753472\n",
      "3_38 train_acc: 0.7955 train_loss: 0.595735\tval_acc: 0.857143 val_loss: 0.3919443 test_acc:0.739583\n",
      "3_40 train_acc: 0.8636 train_loss: 0.413467\tval_acc: 0.875000 val_loss: 0.3671995 test_acc:0.763889\n",
      "3_45 train_acc: 0.7727 train_loss: 0.546825\tval_acc: 0.892857 val_loss: 0.3140374 test_acc:0.781250\n",
      "3_46 train_acc: 0.7727 train_loss: 0.606383\tval_acc: 0.910714 val_loss: 0.3171226 test_acc:0.770833\n",
      "3_47 train_acc: 0.8636 train_loss: 0.319491\tval_acc: 0.910714 val_loss: 0.2618796 test_acc:0.763889\n",
      "3_48 train_acc: 0.8523 train_loss: 0.416948\tval_acc: 0.910714 val_loss: 0.2591698 test_acc:0.795139\n",
      "3_55 train_acc: 0.9091 train_loss: 0.339156\tval_acc: 0.910714 val_loss: 0.2379476 test_acc:0.805556\n",
      "3_60 train_acc: 0.8182 train_loss: 0.445701\tval_acc: 0.928571 val_loss: 0.2485875 test_acc:0.819444\n",
      "3_70 train_acc: 0.8636 train_loss: 0.302295\tval_acc: 0.928571 val_loss: 0.2170819 test_acc:0.833333\n",
      "3_75 train_acc: 0.8864 train_loss: 0.297237\tval_acc: 0.928571 val_loss: 0.2062997 test_acc:0.833333\n",
      "3_76 train_acc: 0.8636 train_loss: 0.367025\tval_acc: 0.928571 val_loss: 0.1818989 test_acc:0.826389\n",
      "3_77 train_acc: 0.9545 train_loss: 0.161929\tval_acc: 0.946429 val_loss: 0.1852702 test_acc:0.836806\n",
      "3_78 train_acc: 0.8750 train_loss: 0.294836\tval_acc: 0.946429 val_loss: 0.1653174 test_acc:0.826389\n",
      "3_89 train_acc: 0.9318 train_loss: 0.130221\tval_acc: 0.946429 val_loss: 0.1451872 test_acc:0.854167\n",
      "3_94 train_acc: 0.9545 train_loss: 0.128452\tval_acc: 0.964286 val_loss: 0.1361344 test_acc:0.850694\n",
      "3_104 train_acc: 0.9659 train_loss: 0.116194\tval_acc: 0.964286 val_loss: 0.1054033 test_acc:0.850694\n",
      "3_109 train_acc: 0.9545 train_loss: 0.182959\tval_acc: 0.964286 val_loss: 0.1001215 test_acc:0.850694\n",
      "3_111 train_acc: 0.9659 train_loss: 0.162058\tval_acc: 0.982143 val_loss: 0.0742725 test_acc:0.843750\n",
      "3_128 train_acc: 0.9205 train_loss: 0.233927\tval_acc: 0.982143 val_loss: 0.0679162 test_acc:0.871528\n",
      "3_129 train_acc: 0.9886 train_loss: 0.103612\tval_acc: 0.982143 val_loss: 0.0669217 test_acc:0.895833\n",
      "3_140 train_acc: 0.9545 train_loss: 0.147790\tval_acc: 0.982143 val_loss: 0.0657621 test_acc:0.864583\n",
      "3_143 train_acc: 0.9773 train_loss: 0.093023\tval_acc: 1.000000 val_loss: 0.0499830 test_acc:0.881944\n",
      "3_147 train_acc: 0.9659 train_loss: 0.145814\tval_acc: 1.000000 val_loss: 0.0447036 test_acc:0.899306\n",
      "3_148 train_acc: 0.9318 train_loss: 0.158483\tval_acc: 1.000000 val_loss: 0.0438138 test_acc:0.892361\n",
      "3_162 train_acc: 0.9432 train_loss: 0.123930\tval_acc: 1.000000 val_loss: 0.0403891 test_acc:0.888889\n",
      "3_179 train_acc: 0.9091 train_loss: 0.184438\tval_acc: 1.000000 val_loss: 0.0403595 test_acc:0.892361\n",
      "3_200 train_acc: 0.9659 train_loss: 0.113604\tval_acc: 1.000000 val_loss: 0.0327323 test_acc:0.895833\n",
      "3_215 train_acc: 0.9318 train_loss: 0.153835\tval_acc: 1.000000 val_loss: 0.0317340 test_acc:0.871528\n",
      "3_221 train_acc: 0.9545 train_loss: 0.105510\tval_acc: 1.000000 val_loss: 0.0206111 test_acc:0.909722\n",
      "3_249 train_acc: 0.9773 train_loss: 0.065609\tval_acc: 1.000000 val_loss: 0.0196575 test_acc:0.906250\n",
      "3_290 train_acc: 0.9659 train_loss: 0.075063\tval_acc: 1.000000 val_loss: 0.0167998 test_acc:0.909722\n",
      "3_308 train_acc: 0.9659 train_loss: 0.104648\tval_acc: 1.000000 val_loss: 0.0137081 test_acc:0.916667\n",
      "3_309 train_acc: 0.9659 train_loss: 0.106462\tval_acc: 1.000000 val_loss: 0.0111084 test_acc:0.916667\n",
      "3_310 train_acc: 0.9773 train_loss: 0.106266\tval_acc: 1.000000 val_loss: 0.0096679 test_acc:0.909722\n",
      "3_366 train_acc: 0.9659 train_loss: 0.077331\tval_acc: 1.000000 val_loss: 0.0092545 test_acc:0.906250\n",
      "3_385 train_acc: 0.9659 train_loss: 0.073809\tval_acc: 1.000000 val_loss: 0.0085958 test_acc:0.916667\n",
      "3_403 train_acc: 0.9659 train_loss: 0.172361\tval_acc: 1.000000 val_loss: 0.0057753 test_acc:0.916667\n",
      "3_404 train_acc: 1.0000 train_loss: 0.026862\tval_acc: 1.000000 val_loss: 0.0054793 test_acc:0.913194\n",
      "3_406 train_acc: 1.0000 train_loss: 0.021968\tval_acc: 1.000000 val_loss: 0.0045131 test_acc:0.916667\n",
      "3_408 train_acc: 0.9886 train_loss: 0.027427\tval_acc: 1.000000 val_loss: 0.0037381 test_acc:0.906250\n",
      "3_420 train_acc: 0.9545 train_loss: 0.094323\tval_acc: 1.000000 val_loss: 0.0022968 test_acc:0.888889\n",
      "3_503 train_acc: 0.9773 train_loss: 0.048843\tval_acc: 1.000000 val_loss: 0.0019609 test_acc:0.916667\n",
      "3_699 train_acc: 0.9773 train_loss: 0.029875\tval_acc: 1.000000 val_loss: 0.0010856 test_acc:0.916667\n",
      "3_776 train_acc: 0.9773 train_loss: 0.059308\tval_acc: 1.000000 val_loss: 0.0007668 test_acc:0.927083\n",
      "3_777 train_acc: 0.9545 train_loss: 0.113541\tval_acc: 1.000000 val_loss: 0.0007142 test_acc:0.920139\n",
      "3_844 train_acc: 1.0000 train_loss: 0.003778\tval_acc: 1.000000 val_loss: 0.0005140 test_acc:0.909722\n",
      "3_856 train_acc: 0.9886 train_loss: 0.026761\tval_acc: 1.000000 val_loss: 0.0004620 test_acc:0.927083\n",
      "3_891 train_acc: 0.9773 train_loss: 0.055773\tval_acc: 1.000000 val_loss: 0.0003613 test_acc:0.930556\n",
      "3_942 train_acc: 0.9886 train_loss: 0.048920\tval_acc: 1.000000 val_loss: 0.0002146 test_acc:0.934028\n",
      "epoch:  942 \tThe test accuracy is: 0.9340277777777778\n",
      " THE BEST ACCURACY IS 0.9340277777777778\tkappa is 0.912037037037037\n",
      "subject 3 duration: 0:36:15.026128\n",
      "3 subject 1 fold mean: \n",
      " accuray      93.402778\n",
      "precision    93.523290\n",
      "recall       93.402778\n",
      "f1           93.411531\n",
      "kappa        91.203704\n",
      "dtype: float64\n",
      "seed is 483\n",
      "Subject 4\n",
      "-------------------- raw train size： (288, 1, 22, 1000) test size： (288, 22, 1000) subject: 4 fold: 1\n",
      "-------------------- train size： (232, 1, 22, 1000) val size： (56, 1, 22, 1000)\n",
      "4_0 train_acc: 0.2955 train_loss: 1.973375\tval_acc: 0.160714 val_loss: 1.5715107 test_acc:0.250000\n",
      "4_1 train_acc: 0.2500 train_loss: 1.689399\tval_acc: 0.214286 val_loss: 1.5348214 test_acc:0.263889\n",
      "4_2 train_acc: 0.2500 train_loss: 1.728769\tval_acc: 0.267857 val_loss: 1.4570886 test_acc:0.270833\n",
      "4_3 train_acc: 0.2614 train_loss: 1.556364\tval_acc: 0.303571 val_loss: 1.4379299 test_acc:0.291667\n",
      "4_4 train_acc: 0.2841 train_loss: 1.626721\tval_acc: 0.339286 val_loss: 1.3815916 test_acc:0.298611\n",
      "4_5 train_acc: 0.3864 train_loss: 1.453228\tval_acc: 0.410714 val_loss: 1.3338034 test_acc:0.298611\n",
      "4_6 train_acc: 0.3750 train_loss: 1.363423\tval_acc: 0.464286 val_loss: 1.2834780 test_acc:0.347222\n",
      "4_9 train_acc: 0.4318 train_loss: 1.312782\tval_acc: 0.464286 val_loss: 1.2398698 test_acc:0.395833\n",
      "4_10 train_acc: 0.4432 train_loss: 1.326966\tval_acc: 0.482143 val_loss: 1.2089564 test_acc:0.416667\n",
      "4_15 train_acc: 0.5341 train_loss: 1.175285\tval_acc: 0.553571 val_loss: 1.1655333 test_acc:0.482639\n",
      "4_18 train_acc: 0.4773 train_loss: 1.227916\tval_acc: 0.571429 val_loss: 1.0963310 test_acc:0.493056\n",
      "4_20 train_acc: 0.5341 train_loss: 1.106645\tval_acc: 0.589286 val_loss: 1.0774606 test_acc:0.486111\n",
      "4_23 train_acc: 0.5909 train_loss: 0.919823\tval_acc: 0.589286 val_loss: 0.9913357 test_acc:0.527778\n",
      "4_27 train_acc: 0.6250 train_loss: 0.895625\tval_acc: 0.589286 val_loss: 0.9850534 test_acc:0.590278\n",
      "4_28 train_acc: 0.6705 train_loss: 0.866080\tval_acc: 0.589286 val_loss: 0.9617628 test_acc:0.548611\n",
      "4_29 train_acc: 0.6818 train_loss: 0.828133\tval_acc: 0.732143 val_loss: 0.9026934 test_acc:0.576389\n",
      "4_33 train_acc: 0.6136 train_loss: 0.870023\tval_acc: 0.750000 val_loss: 0.8317869 test_acc:0.618056\n",
      "4_34 train_acc: 0.6477 train_loss: 0.888967\tval_acc: 0.750000 val_loss: 0.7755563 test_acc:0.614583\n",
      "4_37 train_acc: 0.7273 train_loss: 0.714474\tval_acc: 0.750000 val_loss: 0.7368549 test_acc:0.569444\n",
      "4_39 train_acc: 0.6477 train_loss: 0.827271\tval_acc: 0.750000 val_loss: 0.7304925 test_acc:0.593750\n",
      "4_44 train_acc: 0.7045 train_loss: 0.715389\tval_acc: 0.767857 val_loss: 0.7079116 test_acc:0.614583\n",
      "4_45 train_acc: 0.7045 train_loss: 0.833765\tval_acc: 0.785714 val_loss: 0.6775188 test_acc:0.621528\n",
      "4_46 train_acc: 0.6932 train_loss: 0.709858\tval_acc: 0.821429 val_loss: 0.6647506 test_acc:0.656250\n",
      "4_50 train_acc: 0.7273 train_loss: 0.698160\tval_acc: 0.821429 val_loss: 0.6348013 test_acc:0.652778\n",
      "4_55 train_acc: 0.6477 train_loss: 0.849997\tval_acc: 0.821429 val_loss: 0.6080915 test_acc:0.645833\n",
      "4_60 train_acc: 0.8068 train_loss: 0.513335\tval_acc: 0.821429 val_loss: 0.5517436 test_acc:0.673611\n",
      "4_69 train_acc: 0.7727 train_loss: 0.555206\tval_acc: 0.839286 val_loss: 0.5181081 test_acc:0.677083\n",
      "4_81 train_acc: 0.8068 train_loss: 0.499861\tval_acc: 0.839286 val_loss: 0.5073213 test_acc:0.694444\n",
      "4_84 train_acc: 0.7955 train_loss: 0.511034\tval_acc: 0.839286 val_loss: 0.4871932 test_acc:0.673611\n",
      "4_86 train_acc: 0.8182 train_loss: 0.410195\tval_acc: 0.928571 val_loss: 0.4486601 test_acc:0.694444\n",
      "4_112 train_acc: 0.8636 train_loss: 0.503649\tval_acc: 0.928571 val_loss: 0.4202475 test_acc:0.718750\n",
      "4_133 train_acc: 0.8750 train_loss: 0.506139\tval_acc: 0.928571 val_loss: 0.3449476 test_acc:0.722222\n",
      "4_144 train_acc: 0.9091 train_loss: 0.241459\tval_acc: 0.946429 val_loss: 0.3290380 test_acc:0.732639\n",
      "4_167 train_acc: 0.9091 train_loss: 0.299414\tval_acc: 0.946429 val_loss: 0.2812443 test_acc:0.756944\n",
      "4_182 train_acc: 0.8977 train_loss: 0.253221\tval_acc: 0.946429 val_loss: 0.2674419 test_acc:0.753472\n",
      "4_184 train_acc: 0.9205 train_loss: 0.207389\tval_acc: 0.946429 val_loss: 0.2245655 test_acc:0.750000\n",
      "4_185 train_acc: 0.8409 train_loss: 0.314217\tval_acc: 0.946429 val_loss: 0.2242780 test_acc:0.743056\n",
      "4_193 train_acc: 0.8864 train_loss: 0.280961\tval_acc: 0.964286 val_loss: 0.2443679 test_acc:0.743056\n",
      "4_221 train_acc: 0.9205 train_loss: 0.253008\tval_acc: 0.964286 val_loss: 0.2274251 test_acc:0.750000\n",
      "4_289 train_acc: 0.9432 train_loss: 0.204399\tval_acc: 0.964286 val_loss: 0.1782933 test_acc:0.784722\n",
      "4_298 train_acc: 0.9318 train_loss: 0.162502\tval_acc: 0.964286 val_loss: 0.1753204 test_acc:0.767361\n",
      "4_328 train_acc: 0.9545 train_loss: 0.164517\tval_acc: 0.964286 val_loss: 0.1713953 test_acc:0.781250\n",
      "4_336 train_acc: 0.9091 train_loss: 0.220914\tval_acc: 0.964286 val_loss: 0.1290460 test_acc:0.777778\n",
      "4_363 train_acc: 0.8977 train_loss: 0.293930\tval_acc: 0.964286 val_loss: 0.1273629 test_acc:0.788194\n",
      "4_415 train_acc: 0.9432 train_loss: 0.199640\tval_acc: 0.964286 val_loss: 0.0826131 test_acc:0.802083\n",
      "4_431 train_acc: 0.9545 train_loss: 0.103941\tval_acc: 0.964286 val_loss: 0.0643637 test_acc:0.822917\n",
      "4_467 train_acc: 0.9659 train_loss: 0.075910\tval_acc: 0.982143 val_loss: 0.1220448 test_acc:0.809028\n",
      "4_470 train_acc: 0.9432 train_loss: 0.140103\tval_acc: 0.982143 val_loss: 0.0832402 test_acc:0.805556\n",
      "4_471 train_acc: 0.8977 train_loss: 0.251287\tval_acc: 0.982143 val_loss: 0.0680225 test_acc:0.809028\n",
      "4_517 train_acc: 0.9773 train_loss: 0.077633\tval_acc: 0.982143 val_loss: 0.0673630 test_acc:0.798611\n",
      "4_549 train_acc: 1.0000 train_loss: 0.020497\tval_acc: 0.982143 val_loss: 0.0647788 test_acc:0.812500\n",
      "4_551 train_acc: 0.9773 train_loss: 0.075092\tval_acc: 0.982143 val_loss: 0.0633219 test_acc:0.788194\n",
      "4_552 train_acc: 0.9773 train_loss: 0.044936\tval_acc: 0.982143 val_loss: 0.0571122 test_acc:0.809028\n",
      "4_564 train_acc: 0.9659 train_loss: 0.138474\tval_acc: 0.982143 val_loss: 0.0561834 test_acc:0.822917\n",
      "4_577 train_acc: 0.9545 train_loss: 0.156115\tval_acc: 0.982143 val_loss: 0.0424662 test_acc:0.809028\n",
      "4_596 train_acc: 0.9659 train_loss: 0.067600\tval_acc: 0.982143 val_loss: 0.0387039 test_acc:0.826389\n",
      "4_598 train_acc: 0.9659 train_loss: 0.054828\tval_acc: 0.982143 val_loss: 0.0338204 test_acc:0.798611\n",
      "4_600 train_acc: 0.9886 train_loss: 0.041495\tval_acc: 1.000000 val_loss: 0.0308419 test_acc:0.809028\n",
      "4_613 train_acc: 0.9432 train_loss: 0.155782\tval_acc: 1.000000 val_loss: 0.0255797 test_acc:0.812500\n",
      "4_725 train_acc: 0.9773 train_loss: 0.063984\tval_acc: 1.000000 val_loss: 0.0211723 test_acc:0.819444\n",
      "4_840 train_acc: 0.9886 train_loss: 0.067819\tval_acc: 1.000000 val_loss: 0.0175477 test_acc:0.815972\n",
      "4_897 train_acc: 0.9659 train_loss: 0.049589\tval_acc: 1.000000 val_loss: 0.0144249 test_acc:0.812500\n",
      "4_907 train_acc: 0.9773 train_loss: 0.049780\tval_acc: 1.000000 val_loss: 0.0134875 test_acc:0.815972\n",
      "4_909 train_acc: 0.9773 train_loss: 0.065410\tval_acc: 1.000000 val_loss: 0.0116519 test_acc:0.826389\n",
      "epoch:  909 \tThe test accuracy is: 0.8263888888888888\n",
      " THE BEST ACCURACY IS 0.8263888888888888\tkappa is 0.7685185185185185\n",
      "subject 4 duration: 0:39:56.627929\n",
      "4 subject 1 fold mean: \n",
      " accuray      82.638889\n",
      "precision    83.007655\n",
      "recall       82.638889\n",
      "f1           82.488118\n",
      "kappa        76.851852\n",
      "dtype: float64\n",
      "seed is 248\n",
      "Subject 5\n",
      "-------------------- raw train size： (288, 1, 22, 1000) test size： (288, 22, 1000) subject: 5 fold: 1\n",
      "-------------------- train size： (232, 1, 22, 1000) val size： (56, 1, 22, 1000)\n",
      "5_0 train_acc: 0.2500 train_loss: 1.829338\tval_acc: 0.303571 val_loss: 1.3696629 test_acc:0.319444\n",
      "5_1 train_acc: 0.2841 train_loss: 1.631650\tval_acc: 0.339286 val_loss: 1.3186713 test_acc:0.357639\n",
      "5_2 train_acc: 0.3182 train_loss: 1.591743\tval_acc: 0.446429 val_loss: 1.2507703 test_acc:0.350694\n",
      "5_3 train_acc: 0.3636 train_loss: 1.294756\tval_acc: 0.446429 val_loss: 1.2276809 test_acc:0.347222\n",
      "5_4 train_acc: 0.3295 train_loss: 1.508092\tval_acc: 0.482143 val_loss: 1.1590140 test_acc:0.381944\n",
      "5_5 train_acc: 0.4886 train_loss: 1.321079\tval_acc: 0.607143 val_loss: 1.0836579 test_acc:0.406250\n",
      "5_6 train_acc: 0.3864 train_loss: 1.338751\tval_acc: 0.642857 val_loss: 1.0549644 test_acc:0.423611\n",
      "5_9 train_acc: 0.5000 train_loss: 1.184103\tval_acc: 0.678571 val_loss: 0.9080459 test_acc:0.479167\n",
      "5_10 train_acc: 0.5000 train_loss: 1.183405\tval_acc: 0.732143 val_loss: 0.8828921 test_acc:0.486111\n",
      "5_14 train_acc: 0.5682 train_loss: 1.029909\tval_acc: 0.750000 val_loss: 0.7597645 test_acc:0.559028\n",
      "5_16 train_acc: 0.5795 train_loss: 1.079189\tval_acc: 0.750000 val_loss: 0.7178755 test_acc:0.607639\n",
      "5_19 train_acc: 0.6591 train_loss: 0.809816\tval_acc: 0.767857 val_loss: 0.6575134 test_acc:0.652778\n",
      "5_21 train_acc: 0.7273 train_loss: 0.697363\tval_acc: 0.785714 val_loss: 0.6201280 test_acc:0.656250\n",
      "5_22 train_acc: 0.6705 train_loss: 0.823406\tval_acc: 0.821429 val_loss: 0.6114987 test_acc:0.670139\n",
      "5_26 train_acc: 0.7386 train_loss: 0.625501\tval_acc: 0.839286 val_loss: 0.5751189 test_acc:0.677083\n",
      "5_43 train_acc: 0.7727 train_loss: 0.502358\tval_acc: 0.857143 val_loss: 0.4704643 test_acc:0.687500\n",
      "5_65 train_acc: 0.8750 train_loss: 0.351305\tval_acc: 0.857143 val_loss: 0.4634013 test_acc:0.701389\n",
      "5_66 train_acc: 0.8523 train_loss: 0.349944\tval_acc: 0.857143 val_loss: 0.4095539 test_acc:0.718750\n",
      "5_72 train_acc: 0.8409 train_loss: 0.395871\tval_acc: 0.875000 val_loss: 0.4371216 test_acc:0.701389\n",
      "5_83 train_acc: 0.8750 train_loss: 0.312134\tval_acc: 0.875000 val_loss: 0.3953645 test_acc:0.697917\n",
      "5_84 train_acc: 0.8864 train_loss: 0.202097\tval_acc: 0.875000 val_loss: 0.3938239 test_acc:0.697917\n",
      "5_85 train_acc: 0.8977 train_loss: 0.356743\tval_acc: 0.892857 val_loss: 0.3943843 test_acc:0.725694\n",
      "5_88 train_acc: 0.9432 train_loss: 0.204399\tval_acc: 0.892857 val_loss: 0.3731671 test_acc:0.722222\n",
      "5_119 train_acc: 0.8977 train_loss: 0.251769\tval_acc: 0.892857 val_loss: 0.3386194 test_acc:0.729167\n",
      "5_121 train_acc: 0.9318 train_loss: 0.244774\tval_acc: 0.910714 val_loss: 0.3548611 test_acc:0.722222\n",
      "5_126 train_acc: 0.8409 train_loss: 0.420487\tval_acc: 0.928571 val_loss: 0.3465079 test_acc:0.729167\n",
      "5_129 train_acc: 0.8636 train_loss: 0.300147\tval_acc: 0.928571 val_loss: 0.3078960 test_acc:0.701389\n",
      "5_188 train_acc: 0.9432 train_loss: 0.176932\tval_acc: 0.928571 val_loss: 0.2930518 test_acc:0.673611\n",
      "5_196 train_acc: 0.9545 train_loss: 0.111336\tval_acc: 0.928571 val_loss: 0.2755678 test_acc:0.718750\n",
      "5_210 train_acc: 0.8864 train_loss: 0.239266\tval_acc: 0.928571 val_loss: 0.2559106 test_acc:0.694444\n",
      "5_240 train_acc: 0.9091 train_loss: 0.199068\tval_acc: 0.928571 val_loss: 0.2336402 test_acc:0.729167\n",
      "5_247 train_acc: 0.8864 train_loss: 0.375749\tval_acc: 0.946429 val_loss: 0.1989264 test_acc:0.722222\n",
      "5_277 train_acc: 0.9205 train_loss: 0.216960\tval_acc: 0.946429 val_loss: 0.1954141 test_acc:0.652778\n",
      "5_286 train_acc: 0.9886 train_loss: 0.058307\tval_acc: 0.946429 val_loss: 0.1895332 test_acc:0.718750\n",
      "5_302 train_acc: 0.9545 train_loss: 0.101135\tval_acc: 0.964286 val_loss: 0.1725998 test_acc:0.680556\n",
      "5_329 train_acc: 0.9659 train_loss: 0.169418\tval_acc: 0.964286 val_loss: 0.1520026 test_acc:0.701389\n",
      "5_336 train_acc: 0.9318 train_loss: 0.160081\tval_acc: 0.982143 val_loss: 0.1683446 test_acc:0.663194\n",
      "5_500 train_acc: 0.9659 train_loss: 0.080453\tval_acc: 0.982143 val_loss: 0.1124661 test_acc:0.704861\n",
      "5_535 train_acc: 0.9659 train_loss: 0.096382\tval_acc: 0.982143 val_loss: 0.1078747 test_acc:0.736111\n",
      "5_559 train_acc: 0.9432 train_loss: 0.109645\tval_acc: 0.982143 val_loss: 0.0760566 test_acc:0.680556\n",
      "5_688 train_acc: 0.9659 train_loss: 0.103992\tval_acc: 0.982143 val_loss: 0.0758106 test_acc:0.673611\n",
      "5_716 train_acc: 0.9773 train_loss: 0.093357\tval_acc: 0.982143 val_loss: 0.0754241 test_acc:0.635417\n",
      "5_772 train_acc: 0.9773 train_loss: 0.052667\tval_acc: 0.982143 val_loss: 0.0748822 test_acc:0.690972\n",
      "5_780 train_acc: 0.9432 train_loss: 0.166238\tval_acc: 0.982143 val_loss: 0.0613384 test_acc:0.666667\n",
      "5_802 train_acc: 0.9773 train_loss: 0.126267\tval_acc: 0.982143 val_loss: 0.0601899 test_acc:0.673611\n",
      "5_805 train_acc: 0.9886 train_loss: 0.042667\tval_acc: 0.982143 val_loss: 0.0524645 test_acc:0.645833\n",
      "5_914 train_acc: 0.9432 train_loss: 0.158265\tval_acc: 0.982143 val_loss: 0.0500799 test_acc:0.604167\n",
      "5_916 train_acc: 0.9886 train_loss: 0.027208\tval_acc: 0.982143 val_loss: 0.0500796 test_acc:0.701389\n",
      "5_931 train_acc: 0.9886 train_loss: 0.051936\tval_acc: 0.982143 val_loss: 0.0500286 test_acc:0.711806\n",
      "5_932 train_acc: 0.9773 train_loss: 0.066320\tval_acc: 0.982143 val_loss: 0.0480870 test_acc:0.704861\n",
      "5_934 train_acc: 0.9205 train_loss: 0.124152\tval_acc: 0.982143 val_loss: 0.0419513 test_acc:0.704861\n",
      "5_935 train_acc: 0.9773 train_loss: 0.093304\tval_acc: 0.982143 val_loss: 0.0342787 test_acc:0.684028\n",
      "5_992 train_acc: 0.9545 train_loss: 0.066387\tval_acc: 0.982143 val_loss: 0.0300691 test_acc:0.746528\n",
      "epoch:  992 \tThe test accuracy is: 0.7465277777777778\n",
      " THE BEST ACCURACY IS 0.7465277777777778\tkappa is 0.662037037037037\n",
      "subject 5 duration: 0:45:07.834894\n",
      "5 subject 1 fold mean: \n",
      " accuray      74.652778\n",
      "precision    79.870170\n",
      "recall       74.652778\n",
      "f1           75.007310\n",
      "kappa        66.203704\n",
      "dtype: float64\n",
      "seed is 676\n",
      "Subject 6\n",
      "-------------------- raw train size： (288, 1, 22, 1000) test size： (288, 22, 1000) subject: 6 fold: 1\n",
      "-------------------- train size： (232, 1, 22, 1000) val size： (56, 1, 22, 1000)\n",
      "6_0 train_acc: 0.2500 train_loss: 1.856779\tval_acc: 0.250000 val_loss: 1.4018201 test_acc:0.229167\n",
      "6_1 train_acc: 0.3068 train_loss: 1.651256\tval_acc: 0.321429 val_loss: 1.3346699 test_acc:0.270833\n",
      "6_2 train_acc: 0.3409 train_loss: 1.554343\tval_acc: 0.357143 val_loss: 1.3105875 test_acc:0.312500\n",
      "6_3 train_acc: 0.3523 train_loss: 1.496890\tval_acc: 0.428571 val_loss: 1.2526764 test_acc:0.350694\n",
      "6_4 train_acc: 0.4659 train_loss: 1.402978\tval_acc: 0.517857 val_loss: 1.2100819 test_acc:0.378472\n",
      "6_5 train_acc: 0.3523 train_loss: 1.462478\tval_acc: 0.517857 val_loss: 1.1835462 test_acc:0.395833\n",
      "6_7 train_acc: 0.3068 train_loss: 1.479659\tval_acc: 0.535714 val_loss: 1.1019690 test_acc:0.406250\n",
      "6_11 train_acc: 0.5795 train_loss: 1.077259\tval_acc: 0.553571 val_loss: 1.0001719 test_acc:0.440972\n",
      "6_12 train_acc: 0.4886 train_loss: 1.061265\tval_acc: 0.625000 val_loss: 0.9741960 test_acc:0.447917\n",
      "6_13 train_acc: 0.4659 train_loss: 1.101153\tval_acc: 0.642857 val_loss: 0.9410496 test_acc:0.444444\n",
      "6_14 train_acc: 0.5682 train_loss: 1.072053\tval_acc: 0.642857 val_loss: 0.9142019 test_acc:0.468750\n",
      "6_18 train_acc: 0.5909 train_loss: 0.987861\tval_acc: 0.660714 val_loss: 0.8003066 test_acc:0.486111\n",
      "6_20 train_acc: 0.5682 train_loss: 1.001564\tval_acc: 0.660714 val_loss: 0.7678246 test_acc:0.493056\n",
      "6_21 train_acc: 0.6932 train_loss: 0.852300\tval_acc: 0.660714 val_loss: 0.7482382 test_acc:0.479167\n",
      "6_22 train_acc: 0.6705 train_loss: 0.803043\tval_acc: 0.678571 val_loss: 0.6981762 test_acc:0.496528\n",
      "6_24 train_acc: 0.6250 train_loss: 0.848111\tval_acc: 0.767857 val_loss: 0.7051046 test_acc:0.468750\n",
      "6_28 train_acc: 0.6477 train_loss: 0.832393\tval_acc: 0.785714 val_loss: 0.6286586 test_acc:0.503472\n",
      "6_30 train_acc: 0.7045 train_loss: 0.811936\tval_acc: 0.803571 val_loss: 0.5738879 test_acc:0.503472\n",
      "6_33 train_acc: 0.6818 train_loss: 0.755625\tval_acc: 0.803571 val_loss: 0.5711111 test_acc:0.496528\n",
      "6_38 train_acc: 0.7159 train_loss: 0.595905\tval_acc: 0.803571 val_loss: 0.5140657 test_acc:0.506944\n",
      "6_39 train_acc: 0.6591 train_loss: 0.853406\tval_acc: 0.821429 val_loss: 0.4820551 test_acc:0.510417\n",
      "6_43 train_acc: 0.6932 train_loss: 0.693090\tval_acc: 0.821429 val_loss: 0.4771229 test_acc:0.531250\n",
      "6_46 train_acc: 0.7841 train_loss: 0.553298\tval_acc: 0.839286 val_loss: 0.4583613 test_acc:0.552083\n",
      "6_47 train_acc: 0.7273 train_loss: 0.620109\tval_acc: 0.839286 val_loss: 0.4573071 test_acc:0.524306\n",
      "6_51 train_acc: 0.7955 train_loss: 0.488585\tval_acc: 0.839286 val_loss: 0.4190384 test_acc:0.517361\n",
      "6_56 train_acc: 0.8068 train_loss: 0.422698\tval_acc: 0.875000 val_loss: 0.3975873 test_acc:0.559028\n",
      "6_69 train_acc: 0.8864 train_loss: 0.370354\tval_acc: 0.875000 val_loss: 0.3704095 test_acc:0.593750\n",
      "6_88 train_acc: 0.8068 train_loss: 0.406669\tval_acc: 0.875000 val_loss: 0.3060814 test_acc:0.579861\n",
      "6_91 train_acc: 0.8750 train_loss: 0.386086\tval_acc: 0.875000 val_loss: 0.3031867 test_acc:0.604167\n",
      "6_93 train_acc: 0.9432 train_loss: 0.269096\tval_acc: 0.910714 val_loss: 0.2804061 test_acc:0.579861\n",
      "6_125 train_acc: 0.8295 train_loss: 0.375942\tval_acc: 0.910714 val_loss: 0.2652033 test_acc:0.631944\n",
      "6_137 train_acc: 0.8636 train_loss: 0.363233\tval_acc: 0.910714 val_loss: 0.2225124 test_acc:0.642361\n",
      "6_144 train_acc: 0.8977 train_loss: 0.338583\tval_acc: 0.946429 val_loss: 0.2068857 test_acc:0.649306\n",
      "6_145 train_acc: 0.8977 train_loss: 0.234990\tval_acc: 0.946429 val_loss: 0.1687428 test_acc:0.628472\n",
      "6_167 train_acc: 0.8636 train_loss: 0.379245\tval_acc: 0.964286 val_loss: 0.1328891 test_acc:0.687500\n",
      "6_172 train_acc: 0.8864 train_loss: 0.327743\tval_acc: 0.982143 val_loss: 0.1221653 test_acc:0.628472\n",
      "6_173 train_acc: 0.8523 train_loss: 0.396287\tval_acc: 0.982143 val_loss: 0.1030743 test_acc:0.638889\n",
      "6_192 train_acc: 0.9091 train_loss: 0.239605\tval_acc: 1.000000 val_loss: 0.1172225 test_acc:0.659722\n",
      "6_276 train_acc: 0.9659 train_loss: 0.158311\tval_acc: 1.000000 val_loss: 0.0793151 test_acc:0.690972\n",
      "6_289 train_acc: 0.8977 train_loss: 0.291216\tval_acc: 1.000000 val_loss: 0.0579106 test_acc:0.680556\n",
      "6_313 train_acc: 0.9318 train_loss: 0.127342\tval_acc: 1.000000 val_loss: 0.0574346 test_acc:0.673611\n",
      "6_338 train_acc: 0.9318 train_loss: 0.198348\tval_acc: 1.000000 val_loss: 0.0439272 test_acc:0.697917\n",
      "6_340 train_acc: 0.9545 train_loss: 0.082801\tval_acc: 1.000000 val_loss: 0.0406103 test_acc:0.680556\n",
      "6_405 train_acc: 0.9432 train_loss: 0.181013\tval_acc: 1.000000 val_loss: 0.0399660 test_acc:0.690972\n",
      "6_414 train_acc: 0.9545 train_loss: 0.108175\tval_acc: 1.000000 val_loss: 0.0371632 test_acc:0.684028\n",
      "6_439 train_acc: 0.9659 train_loss: 0.094781\tval_acc: 1.000000 val_loss: 0.0360451 test_acc:0.690972\n",
      "6_448 train_acc: 0.9659 train_loss: 0.117319\tval_acc: 1.000000 val_loss: 0.0355988 test_acc:0.694444\n",
      "6_449 train_acc: 0.9659 train_loss: 0.087239\tval_acc: 1.000000 val_loss: 0.0322875 test_acc:0.718750\n",
      "6_452 train_acc: 0.9545 train_loss: 0.112038\tval_acc: 1.000000 val_loss: 0.0299996 test_acc:0.697917\n",
      "6_455 train_acc: 0.9205 train_loss: 0.173128\tval_acc: 1.000000 val_loss: 0.0265357 test_acc:0.690972\n",
      "6_467 train_acc: 0.9545 train_loss: 0.154353\tval_acc: 1.000000 val_loss: 0.0247536 test_acc:0.715278\n",
      "6_513 train_acc: 0.9432 train_loss: 0.234303\tval_acc: 1.000000 val_loss: 0.0226915 test_acc:0.701389\n",
      "6_576 train_acc: 0.9318 train_loss: 0.145032\tval_acc: 1.000000 val_loss: 0.0216528 test_acc:0.729167\n",
      "6_580 train_acc: 0.9773 train_loss: 0.083409\tval_acc: 1.000000 val_loss: 0.0213419 test_acc:0.694444\n",
      "6_599 train_acc: 0.9659 train_loss: 0.083103\tval_acc: 1.000000 val_loss: 0.0196336 test_acc:0.722222\n",
      "6_697 train_acc: 0.9886 train_loss: 0.054757\tval_acc: 1.000000 val_loss: 0.0140034 test_acc:0.732639\n",
      "6_717 train_acc: 0.9659 train_loss: 0.150830\tval_acc: 1.000000 val_loss: 0.0125421 test_acc:0.722222\n",
      "6_755 train_acc: 0.9659 train_loss: 0.055787\tval_acc: 1.000000 val_loss: 0.0117744 test_acc:0.725694\n",
      "6_767 train_acc: 0.9773 train_loss: 0.053084\tval_acc: 1.000000 val_loss: 0.0079945 test_acc:0.722222\n",
      "6_935 train_acc: 0.9886 train_loss: 0.033215\tval_acc: 1.000000 val_loss: 0.0079770 test_acc:0.753472\n",
      "epoch:  935 \tThe test accuracy is: 0.7534722222222222\n",
      " THE BEST ACCURACY IS 0.7534722222222222\tkappa is 0.6712962962962963\n",
      "subject 6 duration: 0:45:32.503896\n",
      "6 subject 1 fold mean: \n",
      " accuray      75.347222\n",
      "precision    76.444253\n",
      "recall       75.347222\n",
      "f1           75.189325\n",
      "kappa        67.129630\n",
      "dtype: float64\n",
      "seed is 653\n",
      "Subject 7\n",
      "-------------------- raw train size： (288, 1, 22, 1000) test size： (288, 22, 1000) subject: 7 fold: 1\n",
      "-------------------- train size： (232, 1, 22, 1000) val size： (56, 1, 22, 1000)\n",
      "7_0 train_acc: 0.3068 train_loss: 1.711163\tval_acc: 0.339286 val_loss: 1.4655428 test_acc:0.340278\n",
      "7_1 train_acc: 0.4091 train_loss: 1.489037\tval_acc: 0.375000 val_loss: 1.3415262 test_acc:0.336806\n",
      "7_3 train_acc: 0.3977 train_loss: 1.475989\tval_acc: 0.375000 val_loss: 1.1952486 test_acc:0.381944\n",
      "7_4 train_acc: 0.4659 train_loss: 1.225251\tval_acc: 0.535714 val_loss: 1.1157742 test_acc:0.454861\n",
      "7_5 train_acc: 0.4432 train_loss: 1.195886\tval_acc: 0.607143 val_loss: 1.0550835 test_acc:0.486111\n",
      "7_6 train_acc: 0.5568 train_loss: 1.110579\tval_acc: 0.642857 val_loss: 0.9920682 test_acc:0.531250\n",
      "7_7 train_acc: 0.4545 train_loss: 1.157729\tval_acc: 0.678571 val_loss: 0.8953452 test_acc:0.552083\n",
      "7_8 train_acc: 0.6023 train_loss: 0.995828\tval_acc: 0.714286 val_loss: 0.8263178 test_acc:0.583333\n",
      "7_9 train_acc: 0.6250 train_loss: 0.923387\tval_acc: 0.714286 val_loss: 0.7788477 test_acc:0.614583\n",
      "7_10 train_acc: 0.6136 train_loss: 0.784429\tval_acc: 0.767857 val_loss: 0.7240109 test_acc:0.597222\n",
      "7_14 train_acc: 0.7159 train_loss: 0.794771\tval_acc: 0.785714 val_loss: 0.6480492 test_acc:0.638889\n",
      "7_16 train_acc: 0.7614 train_loss: 0.611570\tval_acc: 0.803571 val_loss: 0.6126327 test_acc:0.666667\n",
      "7_19 train_acc: 0.7500 train_loss: 0.631717\tval_acc: 0.803571 val_loss: 0.5998003 test_acc:0.659722\n",
      "7_20 train_acc: 0.7159 train_loss: 0.730011\tval_acc: 0.857143 val_loss: 0.5353989 test_acc:0.659722\n",
      "7_31 train_acc: 0.7614 train_loss: 0.672221\tval_acc: 0.875000 val_loss: 0.4409204 test_acc:0.694444\n",
      "7_40 train_acc: 0.8182 train_loss: 0.558543\tval_acc: 0.875000 val_loss: 0.3909809 test_acc:0.704861\n",
      "7_41 train_acc: 0.7386 train_loss: 0.722389\tval_acc: 0.875000 val_loss: 0.3850827 test_acc:0.708333\n",
      "7_42 train_acc: 0.8295 train_loss: 0.431086\tval_acc: 0.875000 val_loss: 0.3834989 test_acc:0.718750\n",
      "7_43 train_acc: 0.7955 train_loss: 0.523127\tval_acc: 0.892857 val_loss: 0.3847687 test_acc:0.711806\n",
      "7_52 train_acc: 0.7841 train_loss: 0.521064\tval_acc: 0.892857 val_loss: 0.3779095 test_acc:0.715278\n",
      "7_56 train_acc: 0.8068 train_loss: 0.411654\tval_acc: 0.910714 val_loss: 0.3682875 test_acc:0.736111\n",
      "7_60 train_acc: 0.8182 train_loss: 0.462063\tval_acc: 0.928571 val_loss: 0.3670139 test_acc:0.753472\n",
      "7_72 train_acc: 0.8409 train_loss: 0.384362\tval_acc: 0.928571 val_loss: 0.3382840 test_acc:0.743056\n",
      "7_75 train_acc: 0.8523 train_loss: 0.336901\tval_acc: 0.928571 val_loss: 0.3030811 test_acc:0.753472\n",
      "7_87 train_acc: 0.8523 train_loss: 0.367119\tval_acc: 0.928571 val_loss: 0.2736999 test_acc:0.756944\n",
      "7_89 train_acc: 0.8636 train_loss: 0.296255\tval_acc: 0.928571 val_loss: 0.2545930 test_acc:0.760417\n",
      "7_96 train_acc: 0.8636 train_loss: 0.329329\tval_acc: 0.928571 val_loss: 0.2388416 test_acc:0.781250\n",
      "7_97 train_acc: 0.9318 train_loss: 0.276663\tval_acc: 0.928571 val_loss: 0.2170547 test_acc:0.784722\n",
      "7_100 train_acc: 0.8864 train_loss: 0.289004\tval_acc: 0.928571 val_loss: 0.1842827 test_acc:0.781250\n",
      "7_115 train_acc: 0.8523 train_loss: 0.403090\tval_acc: 0.946429 val_loss: 0.2109910 test_acc:0.822917\n",
      "7_116 train_acc: 0.9318 train_loss: 0.245664\tval_acc: 0.946429 val_loss: 0.1827853 test_acc:0.829861\n",
      "7_133 train_acc: 0.9545 train_loss: 0.134500\tval_acc: 0.964286 val_loss: 0.1404146 test_acc:0.777778\n",
      "7_217 train_acc: 0.9545 train_loss: 0.116484\tval_acc: 0.964286 val_loss: 0.1400096 test_acc:0.815972\n",
      "7_218 train_acc: 0.9545 train_loss: 0.120182\tval_acc: 0.964286 val_loss: 0.1217828 test_acc:0.802083\n",
      "7_241 train_acc: 0.9659 train_loss: 0.122379\tval_acc: 0.964286 val_loss: 0.0980649 test_acc:0.888889\n",
      "7_263 train_acc: 0.9659 train_loss: 0.082239\tval_acc: 0.982143 val_loss: 0.1006956 test_acc:0.885417\n",
      "7_301 train_acc: 0.9545 train_loss: 0.069397\tval_acc: 0.982143 val_loss: 0.0784410 test_acc:0.868056\n",
      "7_363 train_acc: 1.0000 train_loss: 0.028536\tval_acc: 0.982143 val_loss: 0.0551332 test_acc:0.885417\n",
      "7_377 train_acc: 0.9659 train_loss: 0.060334\tval_acc: 0.982143 val_loss: 0.0398062 test_acc:0.902778\n",
      "7_378 train_acc: 0.9886 train_loss: 0.026612\tval_acc: 1.000000 val_loss: 0.0326807 test_acc:0.920139\n",
      "7_405 train_acc: 0.9773 train_loss: 0.068583\tval_acc: 1.000000 val_loss: 0.0275408 test_acc:0.885417\n",
      "7_415 train_acc: 0.9773 train_loss: 0.065027\tval_acc: 1.000000 val_loss: 0.0275295 test_acc:0.947917\n",
      "7_477 train_acc: 0.9545 train_loss: 0.170388\tval_acc: 1.000000 val_loss: 0.0239802 test_acc:0.892361\n",
      "7_514 train_acc: 0.9545 train_loss: 0.110630\tval_acc: 1.000000 val_loss: 0.0141675 test_acc:0.881944\n",
      "7_545 train_acc: 1.0000 train_loss: 0.009948\tval_acc: 1.000000 val_loss: 0.0135269 test_acc:0.934028\n",
      "7_546 train_acc: 0.9886 train_loss: 0.017820\tval_acc: 1.000000 val_loss: 0.0113633 test_acc:0.927083\n",
      "7_677 train_acc: 0.9886 train_loss: 0.036488\tval_acc: 1.000000 val_loss: 0.0111174 test_acc:0.909722\n",
      "7_679 train_acc: 0.9659 train_loss: 0.058745\tval_acc: 1.000000 val_loss: 0.0068413 test_acc:0.902778\n",
      "7_755 train_acc: 0.9886 train_loss: 0.024272\tval_acc: 1.000000 val_loss: 0.0064596 test_acc:0.888889\n",
      "7_783 train_acc: 0.9886 train_loss: 0.024839\tval_acc: 1.000000 val_loss: 0.0034437 test_acc:0.913194\n",
      "7_880 train_acc: 0.9886 train_loss: 0.045340\tval_acc: 1.000000 val_loss: 0.0023031 test_acc:0.909722\n",
      "7_930 train_acc: 0.9773 train_loss: 0.035829\tval_acc: 1.000000 val_loss: 0.0017929 test_acc:0.902778\n",
      "7_962 train_acc: 1.0000 train_loss: 0.002361\tval_acc: 1.000000 val_loss: 0.0017874 test_acc:0.916667\n",
      "epoch:  962 \tThe test accuracy is: 0.9166666666666666\n",
      " THE BEST ACCURACY IS 0.9166666666666666\tkappa is 0.8888888888888888\n",
      "subject 7 duration: 0:51:40.373450\n",
      "7 subject 1 fold mean: \n",
      " accuray      91.666667\n",
      "precision    92.355861\n",
      "recall       91.666667\n",
      "f1           91.567994\n",
      "kappa        88.888889\n",
      "dtype: float64\n",
      "seed is 1320\n",
      "Subject 8\n",
      "-------------------- raw train size： (288, 1, 22, 1000) test size： (288, 22, 1000) subject: 8 fold: 1\n",
      "-------------------- train size： (232, 1, 22, 1000) val size： (56, 1, 22, 1000)\n",
      "8_0 train_acc: 0.2841 train_loss: 1.662651\tval_acc: 0.303571 val_loss: 1.4439889 test_acc:0.288194\n",
      "8_1 train_acc: 0.3409 train_loss: 1.406989\tval_acc: 0.410714 val_loss: 1.3972145 test_acc:0.312500\n",
      "8_3 train_acc: 0.3750 train_loss: 1.425527\tval_acc: 0.464286 val_loss: 1.2431581 test_acc:0.381944\n",
      "8_4 train_acc: 0.3864 train_loss: 1.386597\tval_acc: 0.482143 val_loss: 1.1689179 test_acc:0.430556\n",
      "8_5 train_acc: 0.5114 train_loss: 1.135219\tval_acc: 0.517857 val_loss: 1.1356353 test_acc:0.437500\n",
      "8_6 train_acc: 0.4773 train_loss: 1.207466\tval_acc: 0.589286 val_loss: 1.0992700 test_acc:0.434028\n",
      "8_11 train_acc: 0.7045 train_loss: 0.800440\tval_acc: 0.660714 val_loss: 0.8726937 test_acc:0.538194\n",
      "8_12 train_acc: 0.6136 train_loss: 1.061288\tval_acc: 0.696429 val_loss: 0.8467045 test_acc:0.545139\n",
      "8_14 train_acc: 0.7727 train_loss: 0.687471\tval_acc: 0.696429 val_loss: 0.7675222 test_acc:0.565972\n",
      "8_15 train_acc: 0.6477 train_loss: 0.861729\tval_acc: 0.696429 val_loss: 0.6848380 test_acc:0.590278\n",
      "8_16 train_acc: 0.6818 train_loss: 0.721919\tval_acc: 0.732143 val_loss: 0.6820818 test_acc:0.618056\n",
      "8_18 train_acc: 0.7045 train_loss: 0.782481\tval_acc: 0.785714 val_loss: 0.6353045 test_acc:0.638889\n",
      "8_20 train_acc: 0.8068 train_loss: 0.525880\tval_acc: 0.785714 val_loss: 0.5943250 test_acc:0.663194\n",
      "8_21 train_acc: 0.7727 train_loss: 0.567353\tval_acc: 0.785714 val_loss: 0.5791391 test_acc:0.680556\n",
      "8_23 train_acc: 0.8182 train_loss: 0.409649\tval_acc: 0.785714 val_loss: 0.5281609 test_acc:0.677083\n",
      "8_25 train_acc: 0.8068 train_loss: 0.447385\tval_acc: 0.803571 val_loss: 0.5317289 test_acc:0.677083\n",
      "8_26 train_acc: 0.7386 train_loss: 0.645750\tval_acc: 0.821429 val_loss: 0.5240510 test_acc:0.697917\n",
      "8_29 train_acc: 0.8523 train_loss: 0.443642\tval_acc: 0.821429 val_loss: 0.4825647 test_acc:0.708333\n",
      "8_30 train_acc: 0.7841 train_loss: 0.512869\tval_acc: 0.839286 val_loss: 0.4886716 test_acc:0.694444\n",
      "8_34 train_acc: 0.8977 train_loss: 0.289423\tval_acc: 0.839286 val_loss: 0.4141409 test_acc:0.725694\n",
      "8_40 train_acc: 0.8409 train_loss: 0.435965\tval_acc: 0.839286 val_loss: 0.3976948 test_acc:0.760417\n",
      "8_42 train_acc: 0.8523 train_loss: 0.332294\tval_acc: 0.839286 val_loss: 0.3485267 test_acc:0.770833\n",
      "8_43 train_acc: 0.8636 train_loss: 0.370312\tval_acc: 0.839286 val_loss: 0.3105698 test_acc:0.770833\n",
      "8_44 train_acc: 0.8182 train_loss: 0.387833\tval_acc: 0.857143 val_loss: 0.3517733 test_acc:0.774306\n",
      "8_47 train_acc: 0.8636 train_loss: 0.383049\tval_acc: 0.857143 val_loss: 0.3351626 test_acc:0.763889\n",
      "8_48 train_acc: 0.7841 train_loss: 0.525722\tval_acc: 0.875000 val_loss: 0.2929034 test_acc:0.770833\n",
      "8_50 train_acc: 0.8636 train_loss: 0.386451\tval_acc: 0.892857 val_loss: 0.2891293 test_acc:0.795139\n",
      "8_59 train_acc: 0.8864 train_loss: 0.270798\tval_acc: 0.910714 val_loss: 0.2591062 test_acc:0.809028\n",
      "8_70 train_acc: 0.8636 train_loss: 0.266915\tval_acc: 0.910714 val_loss: 0.2382389 test_acc:0.802083\n",
      "8_71 train_acc: 0.8523 train_loss: 0.304571\tval_acc: 0.910714 val_loss: 0.2088323 test_acc:0.777778\n",
      "8_72 train_acc: 0.8523 train_loss: 0.318124\tval_acc: 0.910714 val_loss: 0.2083995 test_acc:0.805556\n",
      "8_75 train_acc: 0.9205 train_loss: 0.286934\tval_acc: 0.910714 val_loss: 0.2045945 test_acc:0.812500\n",
      "8_77 train_acc: 0.9545 train_loss: 0.259575\tval_acc: 0.946429 val_loss: 0.1899295 test_acc:0.815972\n",
      "8_81 train_acc: 0.9091 train_loss: 0.228471\tval_acc: 0.946429 val_loss: 0.1702138 test_acc:0.812500\n",
      "8_83 train_acc: 0.9318 train_loss: 0.195618\tval_acc: 0.946429 val_loss: 0.1684711 test_acc:0.805556\n",
      "8_89 train_acc: 0.9091 train_loss: 0.226426\tval_acc: 0.946429 val_loss: 0.1667264 test_acc:0.819444\n",
      "8_113 train_acc: 0.9659 train_loss: 0.083941\tval_acc: 0.964286 val_loss: 0.1668774 test_acc:0.815972\n",
      "8_131 train_acc: 0.9659 train_loss: 0.081873\tval_acc: 0.964286 val_loss: 0.1322713 test_acc:0.822917\n",
      "8_133 train_acc: 0.9773 train_loss: 0.056026\tval_acc: 0.982143 val_loss: 0.1213669 test_acc:0.795139\n",
      "8_312 train_acc: 0.9886 train_loss: 0.035317\tval_acc: 0.982143 val_loss: 0.0482520 test_acc:0.815972\n",
      "8_404 train_acc: 0.9659 train_loss: 0.069305\tval_acc: 0.982143 val_loss: 0.0363897 test_acc:0.791667\n",
      "8_653 train_acc: 0.9773 train_loss: 0.108988\tval_acc: 1.000000 val_loss: 0.0372694 test_acc:0.847222\n",
      "8_662 train_acc: 0.9545 train_loss: 0.075883\tval_acc: 1.000000 val_loss: 0.0352795 test_acc:0.836806\n",
      "8_815 train_acc: 1.0000 train_loss: 0.011618\tval_acc: 1.000000 val_loss: 0.0314904 test_acc:0.843750\n",
      "8_915 train_acc: 0.9886 train_loss: 0.026316\tval_acc: 1.000000 val_loss: 0.0253375 test_acc:0.836806\n",
      "8_919 train_acc: 0.9886 train_loss: 0.020503\tval_acc: 1.000000 val_loss: 0.0207453 test_acc:0.850694\n",
      "8_923 train_acc: 1.0000 train_loss: 0.009299\tval_acc: 1.000000 val_loss: 0.0118531 test_acc:0.857639\n",
      "8_996 train_acc: 0.9886 train_loss: 0.021052\tval_acc: 1.000000 val_loss: 0.0096010 test_acc:0.840278\n",
      "epoch:  996 \tThe test accuracy is: 0.8402777777777778\n",
      " THE BEST ACCURACY IS 0.8402777777777778\tkappa is 0.787037037037037\n",
      "subject 8 duration: 0:37:36.214245\n",
      "8 subject 1 fold mean: \n",
      " accuray      84.027778\n",
      "precision    84.742816\n",
      "recall       84.027778\n",
      "f1           84.042971\n",
      "kappa        78.703704\n",
      "dtype: float64\n",
      "seed is 1945\n",
      "Subject 9\n",
      "-------------------- raw train size： (288, 1, 22, 1000) test size： (288, 22, 1000) subject: 9 fold: 1\n",
      "-------------------- train size： (232, 1, 22, 1000) val size： (56, 1, 22, 1000)\n",
      "9_0 train_acc: 0.2386 train_loss: 1.900928\tval_acc: 0.250000 val_loss: 1.5526539 test_acc:0.250000\n",
      "9_1 train_acc: 0.2727 train_loss: 1.623251\tval_acc: 0.250000 val_loss: 1.4748439 test_acc:0.243056\n",
      "9_2 train_acc: 0.3864 train_loss: 1.425428\tval_acc: 0.285714 val_loss: 1.3867362 test_acc:0.270833\n",
      "9_3 train_acc: 0.4318 train_loss: 1.344828\tval_acc: 0.303571 val_loss: 1.3387578 test_acc:0.270833\n",
      "9_4 train_acc: 0.3864 train_loss: 1.367126\tval_acc: 0.464286 val_loss: 1.2333620 test_acc:0.312500\n",
      "9_5 train_acc: 0.4091 train_loss: 1.287551\tval_acc: 0.482143 val_loss: 1.1734089 test_acc:0.343750\n",
      "9_7 train_acc: 0.3750 train_loss: 1.364035\tval_acc: 0.571429 val_loss: 1.0630218 test_acc:0.354167\n",
      "9_9 train_acc: 0.5000 train_loss: 1.171473\tval_acc: 0.642857 val_loss: 0.9642504 test_acc:0.437500\n",
      "9_10 train_acc: 0.6591 train_loss: 0.970295\tval_acc: 0.678571 val_loss: 0.8584376 test_acc:0.468750\n",
      "9_13 train_acc: 0.6250 train_loss: 0.955976\tval_acc: 0.732143 val_loss: 0.6918153 test_acc:0.520833\n",
      "9_14 train_acc: 0.7045 train_loss: 0.813631\tval_acc: 0.767857 val_loss: 0.6383811 test_acc:0.545139\n",
      "9_16 train_acc: 0.7045 train_loss: 0.673793\tval_acc: 0.767857 val_loss: 0.5886946 test_acc:0.548611\n",
      "9_17 train_acc: 0.6705 train_loss: 0.710141\tval_acc: 0.767857 val_loss: 0.5620076 test_acc:0.545139\n",
      "9_18 train_acc: 0.7159 train_loss: 0.744501\tval_acc: 0.821429 val_loss: 0.5522365 test_acc:0.552083\n",
      "9_26 train_acc: 0.8409 train_loss: 0.475103\tval_acc: 0.821429 val_loss: 0.4094782 test_acc:0.593750\n",
      "9_32 train_acc: 0.8182 train_loss: 0.573271\tval_acc: 0.821429 val_loss: 0.3700901 test_acc:0.625000\n",
      "9_33 train_acc: 0.8409 train_loss: 0.499381\tval_acc: 0.821429 val_loss: 0.3660993 test_acc:0.642361\n",
      "9_34 train_acc: 0.8182 train_loss: 0.476663\tval_acc: 0.839286 val_loss: 0.3561818 test_acc:0.652778\n",
      "9_35 train_acc: 0.8523 train_loss: 0.421520\tval_acc: 0.857143 val_loss: 0.3302414 test_acc:0.628472\n",
      "9_37 train_acc: 0.8523 train_loss: 0.472897\tval_acc: 0.857143 val_loss: 0.3264884 test_acc:0.618056\n",
      "9_38 train_acc: 0.8295 train_loss: 0.364715\tval_acc: 0.875000 val_loss: 0.2884188 test_acc:0.607639\n",
      "9_40 train_acc: 0.8636 train_loss: 0.356186\tval_acc: 0.892857 val_loss: 0.2939587 test_acc:0.631944\n",
      "9_47 train_acc: 0.8864 train_loss: 0.367739\tval_acc: 0.892857 val_loss: 0.2618215 test_acc:0.670139\n",
      "9_50 train_acc: 0.8636 train_loss: 0.320729\tval_acc: 0.910714 val_loss: 0.2052184 test_acc:0.677083\n",
      "9_52 train_acc: 0.8068 train_loss: 0.418494\tval_acc: 0.928571 val_loss: 0.1890277 test_acc:0.684028\n",
      "9_63 train_acc: 0.8636 train_loss: 0.367919\tval_acc: 0.982143 val_loss: 0.1584231 test_acc:0.677083\n",
      "9_78 train_acc: 0.8750 train_loss: 0.343394\tval_acc: 0.982143 val_loss: 0.1189795 test_acc:0.684028\n",
      "9_79 train_acc: 0.8750 train_loss: 0.322029\tval_acc: 0.982143 val_loss: 0.1068397 test_acc:0.697917\n",
      "9_81 train_acc: 0.9432 train_loss: 0.179135\tval_acc: 0.982143 val_loss: 0.1051150 test_acc:0.694444\n",
      "9_84 train_acc: 0.9318 train_loss: 0.173780\tval_acc: 0.982143 val_loss: 0.0951378 test_acc:0.736111\n",
      "9_85 train_acc: 0.9205 train_loss: 0.187053\tval_acc: 1.000000 val_loss: 0.0922966 test_acc:0.722222\n",
      "9_87 train_acc: 0.9432 train_loss: 0.179961\tval_acc: 1.000000 val_loss: 0.0849134 test_acc:0.697917\n",
      "9_106 train_acc: 0.9205 train_loss: 0.257808\tval_acc: 1.000000 val_loss: 0.0699514 test_acc:0.763889\n",
      "9_113 train_acc: 0.9318 train_loss: 0.189737\tval_acc: 1.000000 val_loss: 0.0579695 test_acc:0.746528\n",
      "9_117 train_acc: 0.9773 train_loss: 0.076088\tval_acc: 1.000000 val_loss: 0.0565240 test_acc:0.753472\n",
      "9_123 train_acc: 0.9205 train_loss: 0.182359\tval_acc: 1.000000 val_loss: 0.0450212 test_acc:0.746528\n",
      "9_153 train_acc: 0.9659 train_loss: 0.102664\tval_acc: 1.000000 val_loss: 0.0349285 test_acc:0.781250\n",
      "9_167 train_acc: 0.9545 train_loss: 0.071641\tval_acc: 1.000000 val_loss: 0.0342700 test_acc:0.767361\n",
      "9_168 train_acc: 0.9659 train_loss: 0.079879\tval_acc: 1.000000 val_loss: 0.0315118 test_acc:0.753472\n",
      "9_210 train_acc: 0.9773 train_loss: 0.112000\tval_acc: 1.000000 val_loss: 0.0159862 test_acc:0.805556\n",
      "9_252 train_acc: 0.9545 train_loss: 0.129734\tval_acc: 1.000000 val_loss: 0.0152992 test_acc:0.809028\n",
      "9_255 train_acc: 0.9659 train_loss: 0.064860\tval_acc: 1.000000 val_loss: 0.0116907 test_acc:0.798611\n",
      "9_256 train_acc: 0.9659 train_loss: 0.107596\tval_acc: 1.000000 val_loss: 0.0113421 test_acc:0.812500\n",
      "9_270 train_acc: 0.9659 train_loss: 0.168605\tval_acc: 1.000000 val_loss: 0.0093493 test_acc:0.812500\n",
      "9_291 train_acc: 0.9773 train_loss: 0.055080\tval_acc: 1.000000 val_loss: 0.0092405 test_acc:0.815972\n",
      "9_360 train_acc: 0.9545 train_loss: 0.073478\tval_acc: 1.000000 val_loss: 0.0055956 test_acc:0.819444\n",
      "9_365 train_acc: 0.9659 train_loss: 0.066270\tval_acc: 1.000000 val_loss: 0.0053855 test_acc:0.788194\n",
      "9_433 train_acc: 0.9659 train_loss: 0.055672\tval_acc: 1.000000 val_loss: 0.0049810 test_acc:0.843750\n",
      "9_462 train_acc: 1.0000 train_loss: 0.016673\tval_acc: 1.000000 val_loss: 0.0044762 test_acc:0.815972\n",
      "9_516 train_acc: 1.0000 train_loss: 0.005431\tval_acc: 1.000000 val_loss: 0.0042976 test_acc:0.836806\n",
      "9_518 train_acc: 0.9545 train_loss: 0.129776\tval_acc: 1.000000 val_loss: 0.0035239 test_acc:0.840278\n",
      "9_582 train_acc: 0.9886 train_loss: 0.043143\tval_acc: 1.000000 val_loss: 0.0034991 test_acc:0.815972\n",
      "9_584 train_acc: 0.9886 train_loss: 0.061264\tval_acc: 1.000000 val_loss: 0.0030133 test_acc:0.826389\n",
      "9_632 train_acc: 1.0000 train_loss: 0.001625\tval_acc: 1.000000 val_loss: 0.0020782 test_acc:0.822917\n",
      "9_683 train_acc: 1.0000 train_loss: 0.003937\tval_acc: 1.000000 val_loss: 0.0017834 test_acc:0.861111\n",
      "9_721 train_acc: 0.9886 train_loss: 0.018135\tval_acc: 1.000000 val_loss: 0.0010568 test_acc:0.836806\n",
      "9_799 train_acc: 1.0000 train_loss: 0.004110\tval_acc: 1.000000 val_loss: 0.0009166 test_acc:0.840278\n",
      "9_821 train_acc: 1.0000 train_loss: 0.007990\tval_acc: 1.000000 val_loss: 0.0004926 test_acc:0.833333\n",
      "9_822 train_acc: 1.0000 train_loss: 0.013553\tval_acc: 1.000000 val_loss: 0.0004036 test_acc:0.805556\n",
      "epoch:  822 \tThe test accuracy is: 0.8055555555555556\n",
      " THE BEST ACCURACY IS 0.8055555555555556\tkappa is 0.7407407407407407\n",
      "subject 9 duration: 0:36:14.525971\n",
      "9 subject 1 fold mean: \n",
      " accuray      80.555556\n",
      "precision    81.964014\n",
      "recall       80.555556\n",
      "f1           80.449257\n",
      "kappa        74.074074\n",
      "dtype: float64\n",
      "**The average Best accuracy is: 81.75154320987654kappa is: 75.66872427983539\n",
      "\n",
      "best epochs:  [848, 862, 942, 909, 992, 935, 962, 996, 822]\n",
      "---------  all result  ---------\n",
      "        accuray  precision     recall         f1      kappa\n",
      "0     87.152778  87.181944  87.152778  87.146795  82.870370\n",
      "1     66.319444  71.223812  66.319444  65.415177  55.092593\n",
      "2     93.402778  93.523290  93.402778  93.411531  91.203704\n",
      "3     82.638889  83.007655  82.638889  82.488118  76.851852\n",
      "4     74.652778  79.870170  74.652778  75.007310  66.203704\n",
      "5     75.347222  76.444253  75.347222  75.189325  67.129630\n",
      "6     91.666667  92.355861  91.666667  91.567994  88.888889\n",
      "7     84.027778  84.742816  84.027778  84.042971  78.703704\n",
      "8     80.555556  81.964014  80.555556  80.449257  74.074074\n",
      "mean  81.751543  83.368202  81.751543  81.635386  75.668724\n",
      "std    8.661631   7.159812   8.661631   8.833309  11.548841\n",
      "****************************************\n",
      "Thu Feb  5 19:23:29 2026\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Zhao, W., Lu, H., Zhang, B. et al. TCANet: a temporal convolutional attention network for motor imagery EEG decoding. Cogn Neurodyn 19, 91 (2025). https://doi.org/10.1007/s11571-025-10275-5\n",
    "\n",
    "Email: zhaowei701@163.com\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "gpu_number = 0\n",
    "gpus = [gpu_number]\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_number)\n",
    "\n",
    "\n",
    "from pandas import ExcelWriter\n",
    "from torchsummary import summary\n",
    "from torch.backends import cudnn\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "cudnn.benchmark = False\n",
    "cudnn.deterministic = True\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from einops import rearrange, reduce, repeat\n",
    "\n",
    "from utils import numberClassChannel\n",
    "from utils import load_data_evaluate\n",
    "from utils import calMetrics\n",
    "from utils import calculatePerClass\n",
    "from utils import numberClassChannel\n",
    "\n",
    "\n",
    "class MSCNet(nn.Module):\n",
    "    def __init__(self, \n",
    "                 f1=16, \n",
    "                 pooling_size=52, \n",
    "                 dropout_rate=0.5, \n",
    "                 number_channel=22):\n",
    "        super().__init__()\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv2d(1, f1, (1, 125), (1, 1), padding='same'),\n",
    "            nn.Conv2d(f1, f1, (number_channel, 1), (1, 1), groups=f1),\n",
    "            nn.BatchNorm2d(f1),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1,pooling_size)), \n",
    "            nn.Dropout(dropout_rate),\n",
    "        )\n",
    "        self.cnn2 = nn.Sequential(\n",
    "            nn.Conv2d(1, f1, (1, 62), (1, 1), padding='same'),\n",
    "            nn.Conv2d(f1, f1, (number_channel, 1), (1, 1), groups=f1),\n",
    "            nn.BatchNorm2d(f1),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1,pooling_size)), \n",
    "            nn.Dropout(dropout_rate),\n",
    "        )        \n",
    "        self.cnn3 = nn.Sequential(\n",
    "            nn.Conv2d(1, f1, (1, 31), (1, 1), padding='same'),\n",
    "            nn.Conv2d(f1, f1, (number_channel, 1), (1, 1), groups=f1),\n",
    "            nn.BatchNorm2d(f1),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1,pooling_size)), \n",
    "            nn.Dropout(dropout_rate),\n",
    "        )\n",
    "        self.projection = nn.Sequential(\n",
    "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        b, _, _, _ = x.shape\n",
    "        x1 = self.cnn1(x)\n",
    "        x2 = self.cnn2(x)\n",
    "        x3 = self.cnn3(x)\n",
    "        #通道方向合并\n",
    "        x = torch.cat([x1, x2, x3], dim=1)\n",
    "        x = self.projection(x)\n",
    "        return x    \n",
    "\n",
    "class _TCNBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    TCN Block with Proper Padding for Causal Convolution\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dimension: int,\n",
    "        depth: int,\n",
    "        kernel_size: int,\n",
    "        filters: int,\n",
    "        drop_prob: float,\n",
    "        activation: nn.Module = nn.ELU,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.activation = activation()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.depth = depth\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.downsample = (\n",
    "            nn.Conv1d(input_dimension, filters, kernel_size=1, bias=False)\n",
    "            if input_dimension != filters\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        for i in range(depth):\n",
    "            dilation = 2**i\n",
    "            padding = (kernel_size - 1) * dilation  # Calculate causal padding\n",
    "            conv_block = nn.Sequential(\n",
    "                CausalConv1d(\n",
    "                    in_channels=input_dimension if i == 0 else filters,\n",
    "                    out_channels=filters,\n",
    "                    kernel_size=kernel_size,\n",
    "                    dilation=dilation,\n",
    "                    # padding=padding,\n",
    "                    bias=False,\n",
    "                ),\n",
    "                nn.BatchNorm1d(filters),\n",
    "                self.activation,\n",
    "                nn.Dropout(self.drop_prob),\n",
    "                CausalConv1d(\n",
    "                    in_channels=filters,\n",
    "                    out_channels=filters,\n",
    "                    kernel_size=kernel_size,\n",
    "                    dilation=dilation,\n",
    "                    # padding=padding,\n",
    "                    bias=False,\n",
    "                ),\n",
    "                nn.BatchNorm1d(filters),\n",
    "                self.activation,\n",
    "                nn.Dropout(self.drop_prob),\n",
    "            )\n",
    "            self.layers.append(conv_block)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x shape: (batch_size, time_steps, input_dimension)\n",
    "        x = x.permute(0, 2, 1)  # (batch_size, input_dimension, time_steps)\n",
    "\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        for index, layer in enumerate(self.layers):\n",
    "            out = layer(x)\n",
    "            out = out + res  # Residual connection\n",
    "            out = self.activation(out)\n",
    "            res = out  # Update residual\n",
    "            x = out  # Update input for next layer\n",
    "\n",
    "        out = out.permute(0, 2, 1)  # (batch_size, time_steps, filters)\n",
    "        return out\n",
    "\n",
    "\n",
    "class CausalConv1d(nn.Conv1d):\n",
    "    \"\"\"\n",
    "    1D Causal Convolution to ensure no information leakage from future timesteps.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Apply padding manually to ensure causality\n",
    "        # print(\"kernel:\", self.kernel_size, self.dilation)\n",
    "        padding = (self.kernel_size[0] - 1) * self.dilation[0]  # Calculate padding size\n",
    "        x = F.pad(x, (padding, 0))  # Only pad on the left (causal padding)\n",
    "        return super().forward(x)\n",
    "\n",
    "class TCANet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # Signal related parameters\n",
    "        n_chans=22,\n",
    "        out_features: int = 4,\n",
    "        n_times=1000,\n",
    "        # Model parameters\n",
    "        activation: nn.Module = nn.ELU,\n",
    "        depth_multiplier: int = 2,\n",
    "        filter_1: int = 8,\n",
    "        kern_length: int = 32,\n",
    "        drop_prob: float = 0.5,\n",
    "        depth: int = 2,\n",
    "        kernel_size: int = 4,\n",
    "        filters: int = 16,\n",
    "        max_norm_const: float = 0.25,\n",
    "        \n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_times = n_times\n",
    "        self.n_chans=n_chans\n",
    "        self.activation = activation\n",
    "        self.drop_prob = drop_prob\n",
    "        self.depth_multiplier = depth_multiplier\n",
    "        self.filter_1 = filter_1\n",
    "        self.kern_length = kern_length\n",
    "        self.depth = depth\n",
    "        self.kernel_size = kernel_size\n",
    "        self.filters = filters\n",
    "        self.max_norm_const = max_norm_const\n",
    "        self.filter_2 = self.filter_1 * self.depth_multiplier\n",
    "        self.out_features = out_features\n",
    "        # EEGNet_TC Block\n",
    "        self.mseegnet = MSCNet(\n",
    "            number_channel=self.n_chans,\n",
    "            dropout_rate=self.drop_prob,\n",
    "            pooling_size=POOLING_SIZE\n",
    "        )\n",
    "\n",
    "        # TCN Block\n",
    "        self.tcn_block = _TCNBlock(\n",
    "            input_dimension=48,\n",
    "            depth=self.depth,\n",
    "            kernel_size=self.kernel_size,\n",
    "            filters=self.filters,\n",
    "            drop_prob=0.25,\n",
    "            activation=self.activation,\n",
    "        )\n",
    "        self.sa = TransformerEncoder(HEADS, DEPTH, self.filters)\n",
    "        self.drop = nn.Dropout(0.25)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.classifier=nn.Linear(self.filters*(1000//POOLING_SIZE), self.out_features,)\n",
    "        self.norm_rate = self.max_norm_const\n",
    "        self.classifier.register_forward_pre_hook(self.apply_max_norm_classifier)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the EEGTCNet model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Input tensor of shape (batch_size, n_chans, n_times).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Output tensor of shape (batch_size, n_outputs).\n",
    "        \"\"\"\n",
    "        x = self.mseegnet(x)  # (batch_size, filter, reduced_time, 1)\n",
    "        x = self.tcn_block(x)  # (batch_size, time_steps, filters)\n",
    "        sa = self.sa(x)\n",
    "        x = self.drop(sa + x)\n",
    "\n",
    "        features = self.flatten(x)\n",
    "        x = self.classifier(features)  # (batch_size, n_outputs)\n",
    "\n",
    "        return x, features           \n",
    "            \n",
    "    def apply_max_norm_classifier(self, module, input):\n",
    "        with torch.no_grad():\n",
    "            norm = self.classifier.weight.data.norm(2, dim=0, keepdim=True)\n",
    "            desired = torch.clamp(norm, max=self.norm_rate)\n",
    "            scale = desired / (norm + 1e-8)\n",
    "            self.classifier.weight.data *= scale    \n",
    "    \n",
    "    \n",
    "  \n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.num_heads = num_heads\n",
    "        self.keys = nn.Linear(emb_size, emb_size)\n",
    "        self.queries = nn.Linear(emb_size, emb_size)\n",
    "        self.values = nn.Linear(emb_size, emb_size)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(emb_size, emb_size)\n",
    "\n",
    "    def forward(self, x: Tensor, mask: Tensor = None) -> Tensor:\n",
    "        queries = rearrange(self.queries(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        keys = rearrange(self.keys(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        values = rearrange(self.values(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)  \n",
    "        if mask is not None:\n",
    "            fill_value = torch.finfo(torch.float32).min\n",
    "            energy.mask_fill(~mask, fill_value)\n",
    "\n",
    "        scaling = self.emb_size ** (1 / 2)\n",
    "        att = F.softmax(energy / scaling, dim=-1)\n",
    "        att = self.att_drop(att)\n",
    "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.projection(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "\n",
    "# PointWise FFN\n",
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self, emb_size, expansion, drop_p):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_size, expansion * emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * emb_size, emb_size),\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, flatten_number, n_classes):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(flatten_number, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn, emb_size, drop_p):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.drop = nn.Dropout(drop_p)\n",
    "        self.layernorm = nn.LayerNorm(emb_size)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        x_input = x\n",
    "        res = self.fn(x, **kwargs)\n",
    "        out = self.layernorm(self.drop(res)+x_input)\n",
    "        return out\n",
    "\n",
    "    \n",
    "class TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 emb_size,\n",
    "                 num_heads=4,\n",
    "                 drop_p=0.5,\n",
    "                 forward_expansion=4,\n",
    "                 forward_drop_p=0.5):\n",
    "        super().__init__(\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                MultiHeadAttention(emb_size, num_heads, drop_p),\n",
    "                ), emb_size, drop_p),\n",
    "\n",
    "            )    \n",
    "        \n",
    "        \n",
    "class TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self, heads, depth, emb_size):\n",
    "        super().__init__(*[TransformerEncoderBlock(emb_size, heads) for _ in range(depth)])\n",
    "\n",
    "\n",
    "\n",
    "class PositioinalEncoding(nn.Module):\n",
    "    def __init__(self, embedding, length=100, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.encoding = nn.Parameter(torch.randn(1, length, embedding))\n",
    "    def forward(self, x): # x-> [batch, embedding, length]\n",
    "        x = x + self.encoding[:, :x.shape[1], :].cuda()\n",
    "        return self.dropout(x)        \n",
    "        \n",
    "    \n",
    "class EEGTransformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 parameters,\n",
    "                 database_type='A', \n",
    "                 \n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "        self.number_class, self.number_channel = numberClassChannel(database_type)\n",
    "        parameters.number_channel = self.number_channel\n",
    "        self.cnn = TCANet(\n",
    "                # Signal related parameters\n",
    "                n_chans=self.number_channel,\n",
    "                out_features=self.number_class,\n",
    "\n",
    "            ) \n",
    "    def forward(self, x):\n",
    "        out, features = self.cnn(x)\n",
    "        return features, out\n",
    "\n",
    "\n",
    "class ExP():\n",
    "    def __init__(self, nsub, data_dir, result_name,\n",
    "                 parameters,\n",
    "                 evaluate_mode = 'LOSO-no',\n",
    "                 dataset_type='A',\n",
    "                 n_fold = 0,\n",
    "                 ):\n",
    "        \n",
    "        super(ExP, self).__init__()\n",
    "        self.n_fold = n_fold\n",
    "        self.dataset_type = dataset_type\n",
    "        self.batch_size = parameters.batch_size\n",
    "        self.lr = parameters.learning_rate\n",
    "        self.b1 = 0.5\n",
    "        self.b2 = 0.999\n",
    "        self.n_epochs = parameters.epochs\n",
    "        self.nSub = nsub\n",
    "        self.nFold = n_fold\n",
    "        self.number_augmentation = parameters.number_aug\n",
    "        self.number_seg = parameters.number_seg\n",
    "        self.root = data_dir\n",
    "        self.result_name = result_name\n",
    "        self.evaluate_mode = evaluate_mode\n",
    "        self.Tensor = torch.cuda.FloatTensor\n",
    "        self.LongTensor = torch.cuda.LongTensor\n",
    "        self.criterion_cls = torch.nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "        self.number_class, self.number_channel = numberClassChannel(dataset_type)\n",
    "        self.model = EEGTransformer(\n",
    "            database_type=self.dataset_type, \n",
    "            parameters = parameters, \n",
    "            ).cuda()\n",
    "        #self.model = nn.DataParallel(self.model, device_ids=gpus)\n",
    "        self.model = self.model.cuda()\n",
    "        self.model_filename = self.result_name + '/model_nsub_{}_nfold_{}.pth'.format(self.nSub, n_fold+1)\n",
    "\n",
    "    def interaug(self, timg, label):  \n",
    "        aug_data = []\n",
    "        aug_label = []\n",
    "        number_records_by_augmentation = self.number_augmentation * int(self.batch_size / self.number_class)\n",
    "        number_segmentation_points = 1000 // self.number_seg\n",
    "        for clsAug in range(self.number_class):\n",
    "            cls_idx = np.where(label == clsAug + 1)\n",
    "            tmp_data = timg[cls_idx]\n",
    "            tmp_label = label[cls_idx]\n",
    "            tmp_aug_data = np.zeros((number_records_by_augmentation, 1, self.number_channel, 1000))\n",
    "            for ri in range(number_records_by_augmentation):\n",
    "                for rj in range(self.number_seg):\n",
    "                    rand_idx = np.random.randint(0, tmp_data.shape[0], self.number_seg)\n",
    "                    tmp_aug_data[ri, :, :, rj * number_segmentation_points:(rj + 1) * number_segmentation_points] = \\\n",
    "                        tmp_data[rand_idx[rj], :, :, rj * number_segmentation_points:(rj + 1) * number_segmentation_points]\n",
    "\n",
    "            aug_data.append(tmp_aug_data)\n",
    "            aug_label.append(tmp_label[:number_records_by_augmentation])\n",
    "        aug_data = np.concatenate(aug_data)\n",
    "        aug_label = np.concatenate(aug_label)\n",
    "        aug_shuffle = np.random.permutation(len(aug_data))\n",
    "        aug_data = aug_data[aug_shuffle, :, :]\n",
    "        aug_label = aug_label[aug_shuffle]\n",
    "\n",
    "        aug_data = torch.from_numpy(aug_data).cuda()\n",
    "        aug_data = aug_data.float()\n",
    "        aug_label = torch.from_numpy(aug_label-1).cuda()\n",
    "        aug_label = aug_label.long()\n",
    "        return aug_data, aug_label\n",
    "\n",
    "\n",
    "\n",
    "    def get_source_data(self):\n",
    "        (self.train_data,    # (batch, channel, length)\n",
    "         self.train_label, \n",
    "         self.test_data, \n",
    "         self.test_label) = load_data_evaluate(self.root, self.dataset_type, self.nSub, mode_evaluate=self.evaluate_mode)\n",
    "\n",
    "        self.train_data = np.expand_dims(self.train_data, axis=1)  # (288, 1, 22, 1000)\n",
    "        self.train_label = np.transpose(self.train_label)  \n",
    "\n",
    "        self.allData = self.train_data\n",
    "        self.allLabel = self.train_label[0]  \n",
    "\n",
    "        shuffle_num = np.random.permutation(len(self.allData))\n",
    "        self.allData = self.allData[shuffle_num, :, :, :]  # (288, 1, 22, 1000)\n",
    "        self.allLabel = self.allLabel[shuffle_num]\n",
    "\n",
    "        print('-'*20, \n",
    "              \"raw train size：\", self.train_data.shape, \n",
    "              \"test size：\", self.test_data.shape, \n",
    "              \"subject:\", self.nSub,\n",
    "              \"fold:\", self.nFold+1)\n",
    "        # self.test_data = np.transpose(self.test_data, (2, 1, 0))\n",
    "        self.test_data = np.expand_dims(self.test_data, axis=1)\n",
    "        self.test_label = np.transpose(self.test_label)\n",
    "\n",
    "        self.testData = self.test_data\n",
    "        self.testLabel = self.test_label[0]\n",
    "        \n",
    "        \n",
    "        \n",
    "        # standardize\n",
    "        target_mean = np.mean(self.allData)\n",
    "        target_std = np.std(self.allData)\n",
    "        self.allData = (self.allData - target_mean) / target_std\n",
    "        self.testData = (self.testData - target_mean) / target_std\n",
    "        \n",
    "        isSaveDataLabel = False #True\n",
    "        if isSaveDataLabel:\n",
    "            np.save(\"./gradm_data/train_data_{}.npy\".format(self.nSub), self.allData)\n",
    "            np.save(\"./gradm_data/train_lable_{}.npy\".format(self.nSub), self.allLabel)\n",
    "            np.save(\"./gradm_data/test_data_{}.npy\".format(self.nSub), self.testData)\n",
    "            np.save(\"./gradm_data/test_label_{}.npy\".format(self.nSub), self.testLabel)\n",
    "\n",
    "        \n",
    "        # data shape: (trial, conv channel, electrode channel, time samples)\n",
    "        return self.allData, self.allLabel, self.testData, self.testLabel\n",
    "\n",
    "    \n",
    "    def test_model(self, model, dataloader):\n",
    "        model.eval()\n",
    "        outputs_list = []\n",
    "        label_list = []\n",
    "        with torch.no_grad():\n",
    "            for i, (img, label) in enumerate(dataloader):\n",
    "                # val model\n",
    "                img = img.type(self.Tensor).cuda()\n",
    "                label = label.type(self.LongTensor).cuda()\n",
    "                _, Cls = model(img)\n",
    "                outputs_list.append(Cls)\n",
    "                del img, Cls\n",
    "                torch.cuda.empty_cache()\n",
    "                label_list.append(label)\n",
    "            \n",
    "        Cls = torch.cat(outputs_list)\n",
    "        val_label = torch.cat(label_list)\n",
    "        val_loss = self.criterion_cls(Cls, val_label)\n",
    "        val_pred = torch.max(Cls, 1)[1]\n",
    "        val_acc = float((val_pred == val_label).cpu().numpy().astype(int).sum()) / float(val_label.size(0))\n",
    "        return val_acc, val_loss, val_pred\n",
    "\n",
    "    \n",
    "\n",
    "    def train(self):\n",
    "        timg, label, test_data, test_label = self.get_source_data()\n",
    "        train_data_list_per_class = []\n",
    "        train_label_list_per_class = []\n",
    "        for clsAug in range(self.number_class):\n",
    "            cls_idx = np.where(label == clsAug + 1)\n",
    "            tmp_data = timg[cls_idx]\n",
    "            tmp_label = label[cls_idx]        \n",
    "            train_data_list_per_class.append(tmp_data)\n",
    "            train_label_list_per_class.append(tmp_label)\n",
    "            \n",
    "        train_data_list = []\n",
    "        train_label_list = []\n",
    "        val_data_list, val_label_list =  [], []\n",
    "        seed = 1234+self.nSub\n",
    "        for clsAug in range(self.number_class):\n",
    "            number_samples = len(train_data_list_per_class[clsAug])\n",
    "            number_test = number_samples // 5      # for validation\n",
    "            # Index is in random order, used to split the training set and test set\n",
    "            index_list = list(range(number_samples))\n",
    "            np.random.seed(seed+clsAug)\n",
    "            # Random index, fetching data based on the index, equivalent to shuffle the data set\n",
    "            index_shuffled = np.random.permutation(index_list)    \n",
    "            if self.n_fold!=4 :\n",
    "                index_val = [i for i in range(self.n_fold*number_test, (self.n_fold+1)*number_test)]\n",
    "            else:\n",
    "                # Since 288 (BCI IV-2a & IV-2b) cannot be divided by 5, the last fold is all the remaining\n",
    "                index_val = [i for i in range(self.n_fold*number_test, number_samples)]\n",
    "\n",
    "            index_train = [i for i in range(number_samples) if i not in index_val]\n",
    "            # Indexes of training and test sets\n",
    "            index_train = index_shuffled[index_train]\n",
    "            index_val = index_shuffled[index_val]   \n",
    "            \n",
    "            train_data_class = train_data_list_per_class[clsAug][index_train]\n",
    "            train_label_class = train_label_list_per_class[clsAug][index_train]\n",
    "            train_data_list.append(train_data_class)\n",
    "            train_label_list.append(train_label_class)\n",
    "            \n",
    "            val_data_class = train_data_list_per_class[clsAug][index_val]\n",
    "            val_label_class = train_label_list_per_class[clsAug][index_val]            \n",
    "            val_data_list.append(val_data_class)\n",
    "            val_label_list.append(val_label_class)        \n",
    "        img = np.concatenate(train_data_list)\n",
    "        label = np.concatenate(train_label_list)\n",
    "        val_data = np.concatenate(val_data_list)\n",
    "        val_label = np.concatenate(val_label_list)\n",
    "      \n",
    "        \n",
    "        print('-'*20, \n",
    "              \"train size：\", img.shape, \n",
    "              \"val size：\", val_data.shape, )\n",
    "        \n",
    "        img = torch.from_numpy(img)\n",
    "        label = torch.from_numpy(label - 1)\n",
    "        dataset = torch.utils.data.TensorDataset(img, label)\n",
    "        self.dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        \n",
    "        val_data = torch.from_numpy(val_data)\n",
    "        val_label = torch.from_numpy(val_label - 1)\n",
    "        val_dataset = torch.utils.data.TensorDataset(val_data, val_label)\n",
    "        self.val_dataloader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "                \n",
    "        \n",
    "        test_data = torch.from_numpy(test_data)\n",
    "        test_label = torch.from_numpy(test_label - 1)\n",
    "        test_dataset = torch.utils.data.TensorDataset(test_data, test_label)\n",
    "        self.test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        # Optimizers\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, betas=(self.b1, self.b2),)\n",
    "\n",
    "        test_data = Variable(test_data.type(self.Tensor))\n",
    "        test_label = Variable(test_label.type(self.LongTensor))\n",
    "        best_epoch = 0\n",
    "        num = 0\n",
    "        min_loss = 100\n",
    "        max_acc = 0\n",
    "        # recording train_acc, train_loss, test_acc, test_loss\n",
    "        result_process = []\n",
    "        # Train the cnn model\n",
    "        for e in range(self.n_epochs):\n",
    "            epoch_process = {}\n",
    "            epoch_process['epoch'] = e\n",
    "            # in_epoch = time.time()\n",
    "            self.model.train()\n",
    "            outputs_list = []\n",
    "            label_list = []\n",
    "            for i, (img, label) in enumerate(self.dataloader):\n",
    "                number_sample = img.shape[0]\n",
    "                \n",
    "                # split raw train dataset into real train dataset and validate dataset\n",
    "                train_data = img\n",
    "                train_label = label\n",
    "\n",
    "                \n",
    "                # real train dataset\n",
    "                img = Variable(train_data.type(self.Tensor))\n",
    "                label = Variable(train_label.type(self.LongTensor))\n",
    "                \n",
    "                # data augmentation\n",
    "                aug_data, aug_label = self.interaug(self.allData, self.allLabel)\n",
    "                # concat real train dataset and generate aritifical train dataset\n",
    "                img = torch.cat((img, aug_data))\n",
    "                label = torch.cat((label, aug_label))\n",
    "\n",
    "                # training model\n",
    "                features, outputs = self.model(img)\n",
    "                outputs_list.append(outputs)\n",
    "                label_list.append(label)\n",
    "                # print(\"train outputs: \", outputs.shape, type(outputs))\n",
    "                # print(features.size())\n",
    "                loss = self.criterion_cls(outputs, label) \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            del img\n",
    "            torch.cuda.empty_cache()\n",
    "            # out_epoch = time.time()\n",
    "            # test process\n",
    "            if (e + 1) % 1 == 0:\n",
    "                self.model.eval()\n",
    "                # validate model\n",
    "                val_acc, val_loss, _ = self.test_model(self.model, self.val_dataloader)\n",
    "                \n",
    "                epoch_process['val_acc'] = val_acc                \n",
    "                epoch_process['val_loss'] = val_loss.detach().cpu().numpy()  \n",
    "                \n",
    "                train_pred = torch.max(outputs, 1)[1]\n",
    "                train_acc = float((train_pred == label).cpu().numpy().astype(int).sum()) / float(label.size(0))\n",
    "                epoch_process['train_acc'] = train_acc\n",
    "                epoch_process['train_loss'] = loss.detach().cpu().numpy()\n",
    "\n",
    "                num = num + 1\n",
    "\n",
    "                # if min_loss>val_loss:                \n",
    "                if max_acc<val_acc or (max_acc==val_acc and min_loss>val_loss):\n",
    "                    max_acc = val_acc\n",
    "                    min_loss = val_loss\n",
    "                    \n",
    "                    best_epoch = e\n",
    "                    epoch_process['epoch'] = e\n",
    "                    torch.save(self.model, self.model_filename)\n",
    "                    test_acc, test_loss, y_pred  = self.test_model(self.model, self.test_dataloader)\n",
    "                    print(\"{}_{} train_acc: {:.4f} train_loss: {:.6f}\\tval_acc: {:.6f} val_loss: {:.7f} test_acc:{:.6f}\".format(self.nSub,\n",
    "                                                                                           epoch_process['epoch'],\n",
    "                                                                                           epoch_process['train_acc'],\n",
    "                                                                                           epoch_process['train_loss'],\n",
    "                                                                                           epoch_process['val_acc'],\n",
    "                                                                                           epoch_process['val_loss'],\n",
    "                                                                                           test_acc                                     \n",
    "                                                                                        ))\n",
    "            \n",
    "                \n",
    "            result_process.append(epoch_process)  \n",
    "\n",
    "        \n",
    "        # load model for test\n",
    "        self.model.eval()\n",
    "        self.model = torch.load(self.model_filename).cuda()\n",
    "        outputs_list = []\n",
    "        with torch.no_grad():\n",
    "            for i, (img, label) in enumerate(self.test_dataloader):\n",
    "                img_test = Variable(img.type(self.Tensor)).cuda()\n",
    "                # label_test = Variable(label.type(self.LongTensor))\n",
    "\n",
    "                # test model\n",
    "                features, outputs = self.model(img_test)\n",
    "                val_pred = torch.max(outputs, 1)[1]\n",
    "                outputs_list.append(outputs)\n",
    "        outputs = torch.cat(outputs_list) \n",
    "        y_pred = torch.max(outputs, 1)[1]\n",
    "        \n",
    "        \n",
    "        test_acc = float((y_pred == test_label).cpu().numpy().astype(int).sum()) / float(test_label.size(0))\n",
    "        \n",
    "        print(\"epoch: \", best_epoch, '\\tThe test accuracy is:', test_acc)\n",
    "\n",
    "\n",
    "        df_process = pd.DataFrame(result_process)\n",
    "\n",
    "        return test_acc, test_label, y_pred, df_process, best_epoch, outputs\n",
    "        # writer.close()\n",
    "        \n",
    "\n",
    "def main(dirs,           \n",
    "         paramters,\n",
    "         evaluate_mode = 'subject-dependent', \n",
    "         dataset_type='A',    # A->'BCI IV2a', B->'BCI IV2b'\n",
    "         ):\n",
    "\n",
    "    if not os.path.exists(dirs):\n",
    "        os.makedirs(dirs)\n",
    "    result_write_metric = ExcelWriter(dirs+\"/result_metric.xlsx\")\n",
    "    result_metric_dict = {}\n",
    "    y_true_pred_dict = { }\n",
    "\n",
    "    process_write = ExcelWriter(dirs+\"/process_train.xlsx\")\n",
    "    pred_true_write = ExcelWriter(dirs+\"/pred_true.xlsx\")\n",
    "    pred_softmax = ExcelWriter(dirs+\"/pred_softmax.xlsx\")\n",
    "    subjects_result = []\n",
    "    \n",
    "    \n",
    "    best_epochs = []\n",
    "    result_fold = []\n",
    "    for i in range(paramters.subject_number):      \n",
    "        starttime = datetime.datetime.now()\n",
    "        seed_n = np.random.randint(2024)\n",
    "        print('seed is ' + str(seed_n))\n",
    "        random.seed(seed_n)\n",
    "        np.random.seed(seed_n)\n",
    "        torch.manual_seed(seed_n)\n",
    "        torch.cuda.manual_seed(seed_n)\n",
    "        torch.cuda.manual_seed_all(seed_n)\n",
    "        index_round =0\n",
    "        print('Subject %d' % (i+1))\n",
    "\n",
    "        subjects_result_fold = []\n",
    "        # If you want to do cross validation, this should be set to 5. Currently, \n",
    "        for n_fold in range(1):     #  only 20% is used as the validation set, and only 1.\n",
    "            exp = ExP(i + 1, DATA_DIR, dirs, \n",
    "                      paramters,\n",
    "                      evaluate_mode = evaluate_mode,\n",
    "                      dataset_type=dataset_type,\n",
    "                      n_fold=n_fold,\n",
    "                      )\n",
    "\n",
    "            testAcc, Y_true, Y_pred, df_process, best_epoch,pred_output = exp.train()\n",
    "            probs = torch.softmax(pred_output, dim=1).cpu().numpy()\n",
    "            df_probs = pd.DataFrame(probs)\n",
    "            df_probs.to_excel(pred_softmax, sheet_name=str(i+1)+'_'+str(n_fold))\n",
    "            true_cpu = Y_true.cpu().numpy().astype(int)\n",
    "            pred_cpu = Y_pred.cpu().numpy().astype(int)\n",
    "            df_pred_true = pd.DataFrame({'pred': pred_cpu, 'true': true_cpu})\n",
    "            df_pred_true.to_excel(pred_true_write, sheet_name=str(i+1)+'_'+str(n_fold))\n",
    "            y_true_pred_dict[i] = df_pred_true\n",
    "\n",
    "            accuracy, precison, recall, f1, kappa = calMetrics(true_cpu, pred_cpu)\n",
    "            subject_result = {'accuray': accuracy*100,\n",
    "                              'precision': precison*100,\n",
    "                              'recall': recall*100,\n",
    "                              'f1': f1*100, \n",
    "                              'kappa': kappa*100\n",
    "                              }\n",
    "            df_process.to_excel(process_write, sheet_name=str(i+1)+'_'+str(n_fold))\n",
    "            best_epochs.append(best_epoch)\n",
    "            print(' THE BEST ACCURACY IS ' + str(testAcc) + \"\\tkappa is \" + str(kappa) )\n",
    "    \n",
    "\n",
    "            endtime = datetime.datetime.now()\n",
    "            print('subject %d duration: '%(i+1) + str(endtime - starttime))\n",
    "\n",
    "            if i == 0:\n",
    "                yt = Y_true\n",
    "                yp = Y_pred\n",
    "            else:\n",
    "                yt = torch.cat((yt, Y_true))\n",
    "                yp = torch.cat((yp, Y_pred))\n",
    "            subjects_result_fold.append(subject_result)\n",
    "            df_result_fold = pd.DataFrame(subjects_result)\n",
    "            \n",
    "        df = pd.DataFrame(subjects_result_fold)\n",
    "        df.to_excel(result_write_metric, index=False,  sheet_name=str(i+1))\n",
    "        result_fold_mean = df.mean()\n",
    "        print(\"{} subject {} fold mean: \\n {}\".format(i+1, n_fold+1, result_fold_mean))\n",
    "        subjects_result.append(result_fold_mean)\n",
    "    df_result = pd.DataFrame(subjects_result)\n",
    "    process_write.close()\n",
    "    pred_true_write.close()\n",
    "    pred_softmax.close()\n",
    "\n",
    "\n",
    "    print('**The average Best accuracy is: ' + str(df_result['accuray'].mean()) + \"kappa is: \" + str(df_result['kappa'].mean()) + \"\\n\" )\n",
    "    print(\"best epochs: \", best_epochs)\n",
    "    #df_result.to_excel(result_write_metric, index=False)\n",
    "    result_metric_dict = df_result\n",
    "\n",
    "    mean = df_result.mean(axis=0)\n",
    "    mean.name = 'mean'\n",
    "    std = df_result.std(axis=0)\n",
    "    std.name = 'std'\n",
    "    df_result = pd.concat([df_result, pd.DataFrame(mean).T, pd.DataFrame(std).T])\n",
    "    \n",
    "    df_result.to_excel(result_write_metric, index=False, sheet_name='mean')\n",
    "    print('-'*9, ' all result ', '-'*9)\n",
    "    print(df_result)\n",
    "    \n",
    "    print(\"*\"*40)\n",
    "\n",
    "    result_write_metric.close()\n",
    "\n",
    "    \n",
    "    return result_metric_dict\n",
    "\n",
    "\n",
    "class Parameters():\n",
    "    def __init__(self, dropout_rate):\n",
    "        self.heads = 8\n",
    "        self.depth = 5\n",
    "        self.emb_size = 16*3\n",
    "        self.f1 = 16\n",
    "        self.pooling_size = 52\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.subject_number = 9\n",
    "        self.learning_rate = 0.001\n",
    "        self.batch_size = 72 \n",
    "        self.epochs=1000\n",
    "        self.number_aug=1\n",
    "        # The actual number of training batches is: self.batch_size*(1+self.number_aug)\n",
    "        self.number_seg=8\n",
    "        self.gpus=gpus        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #----------------------------------------\n",
    "    DATA_DIR = r'G:/PhD/TCANet/datasets/BCI-ok-2a/mymat_raw/'\n",
    "    EVALUATE_MODE = 'LOSO-No' # leaving one subject out subject-dependent  subject-indenpedent\n",
    "\n",
    "    TYPE = 'A'\n",
    "    if EVALUATE_MODE!='LOSO':\n",
    "        CNN_DROPOUT_RATE = 0.5\n",
    "    else:\n",
    "        CNN_DROPOUT_RATE = 0.25    \n",
    "    \n",
    "    parameters = Parameters(CNN_DROPOUT_RATE)\n",
    "    POOLING_SIZE = 56\n",
    "    HEADS = 2\n",
    "    DEPTH = 6\n",
    "\n",
    "    number_class, number_channel = numberClassChannel(TYPE)\n",
    "    RESULT_NAME = \"TCANet_{}\".format(TYPE)\n",
    "\n",
    "    sModel = EEGTransformer(\n",
    "        database_type=TYPE, \n",
    "        parameters = parameters,  \n",
    "        ).cuda()\n",
    "    summary(sModel, (1, number_channel, 1000)) \n",
    "\n",
    "    print(time.asctime(time.localtime(time.time())))\n",
    "\n",
    "    result = main(RESULT_NAME,\n",
    "                  parameters,\n",
    "                    evaluate_mode = EVALUATE_MODE,\n",
    "                    dataset_type=TYPE,\n",
    "                  )\n",
    "    print(time.asctime(time.localtime(time.time())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33a66ef-37f8-47ea-9e68-93ca5eedba46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
