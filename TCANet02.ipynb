{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f04c528-da9d-4112-a0f8-1a74ece51547",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 41\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meinops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Rearrange, Reduce\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meinops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rearrange, reduce, repeat\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m numberClassChannel\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_data_evaluate\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m calMetrics\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Zhao, W., Lu, H., Zhang, B. et al. TCANet: a temporal convolutional attention network for motor imagery EEG decoding. Cogn Neurodyn 19, 91 (2025). https://doi.org/10.1007/s11571-025-10275-5\n",
    "\n",
    "Email: zhaowei701@163.com\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "gpu_number = 0\n",
    "gpus = [gpu_number]\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_number)\n",
    "\n",
    "\n",
    "from pandas import ExcelWriter\n",
    "from torchsummary import summary\n",
    "from torch.backends import cudnn\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "cudnn.benchmark = False\n",
    "cudnn.deterministic = True\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from einops import rearrange, reduce, repeat\n",
    "\n",
    "from utils import numberClassChannel\n",
    "from utils import load_data_evaluate\n",
    "from utils import calMetrics\n",
    "from utils import calculatePerClass\n",
    "from utils import numberClassChannel\n",
    "\n",
    "\n",
    "class MSCNet(nn.Module):\n",
    "    def __init__(self, \n",
    "                 f1=16, \n",
    "                 pooling_size=52, \n",
    "                 dropout_rate=0.5, \n",
    "                 number_channel=22):\n",
    "        super().__init__()\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv2d(1, f1, (1, 125), (1, 1), padding='same'),\n",
    "            nn.Conv2d(f1, f1, (number_channel, 1), (1, 1), groups=f1),\n",
    "            nn.BatchNorm2d(f1),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1,pooling_size)), \n",
    "            nn.Dropout(dropout_rate),\n",
    "        )\n",
    "        self.cnn2 = nn.Sequential(\n",
    "            nn.Conv2d(1, f1, (1, 62), (1, 1), padding='same'),\n",
    "            nn.Conv2d(f1, f1, (number_channel, 1), (1, 1), groups=f1),\n",
    "            nn.BatchNorm2d(f1),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1,pooling_size)), \n",
    "            nn.Dropout(dropout_rate),\n",
    "        )        \n",
    "        self.cnn3 = nn.Sequential(\n",
    "            nn.Conv2d(1, f1, (1, 31), (1, 1), padding='same'),\n",
    "            nn.Conv2d(f1, f1, (number_channel, 1), (1, 1), groups=f1),\n",
    "            nn.BatchNorm2d(f1),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1,pooling_size)), \n",
    "            nn.Dropout(dropout_rate),\n",
    "        )\n",
    "        self.projection = nn.Sequential(\n",
    "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        b, _, _, _ = x.shape\n",
    "        x1 = self.cnn1(x)\n",
    "        x2 = self.cnn2(x)\n",
    "        x3 = self.cnn3(x)\n",
    "        #通道方向合并\n",
    "        x = torch.cat([x1, x2, x3], dim=1)\n",
    "        x = self.projection(x)\n",
    "        return x    \n",
    "\n",
    "class _TCNBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    TCN Block with Proper Padding for Causal Convolution\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dimension: int,\n",
    "        depth: int,\n",
    "        kernel_size: int,\n",
    "        filters: int,\n",
    "        drop_prob: float,\n",
    "        activation: nn.Module = nn.ELU,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.activation = activation()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.depth = depth\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.downsample = (\n",
    "            nn.Conv1d(input_dimension, filters, kernel_size=1, bias=False)\n",
    "            if input_dimension != filters\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        for i in range(depth):\n",
    "            dilation = 2**i\n",
    "            padding = (kernel_size - 1) * dilation  # Calculate causal padding\n",
    "            conv_block = nn.Sequential(\n",
    "                CausalConv1d(\n",
    "                    in_channels=input_dimension if i == 0 else filters,\n",
    "                    out_channels=filters,\n",
    "                    kernel_size=kernel_size,\n",
    "                    dilation=dilation,\n",
    "                    # padding=padding,\n",
    "                    bias=False,\n",
    "                ),\n",
    "                nn.BatchNorm1d(filters),\n",
    "                self.activation,\n",
    "                nn.Dropout(self.drop_prob),\n",
    "                CausalConv1d(\n",
    "                    in_channels=filters,\n",
    "                    out_channels=filters,\n",
    "                    kernel_size=kernel_size,\n",
    "                    dilation=dilation,\n",
    "                    # padding=padding,\n",
    "                    bias=False,\n",
    "                ),\n",
    "                nn.BatchNorm1d(filters),\n",
    "                self.activation,\n",
    "                nn.Dropout(self.drop_prob),\n",
    "            )\n",
    "            self.layers.append(conv_block)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x shape: (batch_size, time_steps, input_dimension)\n",
    "        x = x.permute(0, 2, 1)  # (batch_size, input_dimension, time_steps)\n",
    "\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        for index, layer in enumerate(self.layers):\n",
    "            out = layer(x)\n",
    "            out = out + res  # Residual connection\n",
    "            out = self.activation(out)\n",
    "            res = out  # Update residual\n",
    "            x = out  # Update input for next layer\n",
    "\n",
    "        out = out.permute(0, 2, 1)  # (batch_size, time_steps, filters)\n",
    "        return out\n",
    "\n",
    "\n",
    "class CausalConv1d(nn.Conv1d):\n",
    "    \"\"\"\n",
    "    1D Causal Convolution to ensure no information leakage from future timesteps.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Apply padding manually to ensure causality\n",
    "        # print(\"kernel:\", self.kernel_size, self.dilation)\n",
    "        padding = (self.kernel_size[0] - 1) * self.dilation[0]  # Calculate padding size\n",
    "        x = F.pad(x, (padding, 0))  # Only pad on the left (causal padding)\n",
    "        return super().forward(x)\n",
    "\n",
    "class TCANet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # Signal related parameters\n",
    "        n_chans=22,\n",
    "        out_features: int = 4,\n",
    "        n_times=1000,\n",
    "        # Model parameters\n",
    "        activation: nn.Module = nn.ELU,\n",
    "        depth_multiplier: int = 2,\n",
    "        filter_1: int = 8,\n",
    "        kern_length: int = 32,\n",
    "        drop_prob: float = 0.5,\n",
    "        depth: int = 2,\n",
    "        kernel_size: int = 4,\n",
    "        filters: int = 16,\n",
    "        max_norm_const: float = 0.25,\n",
    "        \n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_times = n_times\n",
    "        self.n_chans=n_chans\n",
    "        self.activation = activation\n",
    "        self.drop_prob = drop_prob\n",
    "        self.depth_multiplier = depth_multiplier\n",
    "        self.filter_1 = filter_1\n",
    "        self.kern_length = kern_length\n",
    "        self.depth = depth\n",
    "        self.kernel_size = kernel_size\n",
    "        self.filters = filters\n",
    "        self.max_norm_const = max_norm_const\n",
    "        self.filter_2 = self.filter_1 * self.depth_multiplier\n",
    "        self.out_features = out_features\n",
    "        # EEGNet_TC Block\n",
    "        self.mseegnet = MSCNet(\n",
    "            number_channel=self.n_chans,\n",
    "            dropout_rate=self.drop_prob,\n",
    "            pooling_size=POOLING_SIZE\n",
    "        )\n",
    "\n",
    "        # TCN Block\n",
    "        self.tcn_block = _TCNBlock(\n",
    "            input_dimension=48,\n",
    "            depth=self.depth,\n",
    "            kernel_size=self.kernel_size,\n",
    "            filters=self.filters,\n",
    "            drop_prob=0.25,\n",
    "            activation=self.activation,\n",
    "        )\n",
    "        self.sa = TransformerEncoder(HEADS, DEPTH, self.filters)\n",
    "        self.drop = nn.Dropout(0.25)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.classifier=nn.Linear(self.filters*(1000//POOLING_SIZE), self.out_features,)\n",
    "        self.norm_rate = self.max_norm_const\n",
    "        self.classifier.register_forward_pre_hook(self.apply_max_norm_classifier)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the EEGTCNet model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Input tensor of shape (batch_size, n_chans, n_times).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Output tensor of shape (batch_size, n_outputs).\n",
    "        \"\"\"\n",
    "        x = self.mseegnet(x)  # (batch_size, filter, reduced_time, 1)\n",
    "        x = self.tcn_block(x)  # (batch_size, time_steps, filters)\n",
    "        sa = self.sa(x)\n",
    "        x = self.drop(sa + x)\n",
    "\n",
    "        features = self.flatten(x)\n",
    "        x = self.classifier(features)  # (batch_size, n_outputs)\n",
    "\n",
    "        return x, features           \n",
    "            \n",
    "    def apply_max_norm_classifier(self, module, input):\n",
    "        with torch.no_grad():\n",
    "            norm = self.classifier.weight.data.norm(2, dim=0, keepdim=True)\n",
    "            desired = torch.clamp(norm, max=self.norm_rate)\n",
    "            scale = desired / (norm + 1e-8)\n",
    "            self.classifier.weight.data *= scale    \n",
    "    \n",
    "    \n",
    "  \n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.num_heads = num_heads\n",
    "        self.keys = nn.Linear(emb_size, emb_size)\n",
    "        self.queries = nn.Linear(emb_size, emb_size)\n",
    "        self.values = nn.Linear(emb_size, emb_size)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(emb_size, emb_size)\n",
    "\n",
    "    def forward(self, x: Tensor, mask: Tensor = None) -> Tensor:\n",
    "        queries = rearrange(self.queries(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        keys = rearrange(self.keys(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        values = rearrange(self.values(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)  \n",
    "        if mask is not None:\n",
    "            fill_value = torch.finfo(torch.float32).min\n",
    "            energy.mask_fill(~mask, fill_value)\n",
    "\n",
    "        scaling = self.emb_size ** (1 / 2)\n",
    "        att = F.softmax(energy / scaling, dim=-1)\n",
    "        att = self.att_drop(att)\n",
    "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.projection(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "\n",
    "# PointWise FFN\n",
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self, emb_size, expansion, drop_p):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_size, expansion * emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * emb_size, emb_size),\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, flatten_number, n_classes):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(flatten_number, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn, emb_size, drop_p):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.drop = nn.Dropout(drop_p)\n",
    "        self.layernorm = nn.LayerNorm(emb_size)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        x_input = x\n",
    "        res = self.fn(x, **kwargs)\n",
    "        out = self.layernorm(self.drop(res)+x_input)\n",
    "        return out\n",
    "\n",
    "    \n",
    "class TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 emb_size,\n",
    "                 num_heads=4,\n",
    "                 drop_p=0.5,\n",
    "                 forward_expansion=4,\n",
    "                 forward_drop_p=0.5):\n",
    "        super().__init__(\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                MultiHeadAttention(emb_size, num_heads, drop_p),\n",
    "                ), emb_size, drop_p),\n",
    "\n",
    "            )    \n",
    "        \n",
    "        \n",
    "class TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self, heads, depth, emb_size):\n",
    "        super().__init__(*[TransformerEncoderBlock(emb_size, heads) for _ in range(depth)])\n",
    "\n",
    "\n",
    "\n",
    "class PositioinalEncoding(nn.Module):\n",
    "    def __init__(self, embedding, length=100, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.encoding = nn.Parameter(torch.randn(1, length, embedding))\n",
    "    def forward(self, x): # x-> [batch, embedding, length]\n",
    "        x = x + self.encoding[:, :x.shape[1], :].cuda()\n",
    "        return self.dropout(x)        \n",
    "        \n",
    "    \n",
    "class EEGTransformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 parameters,\n",
    "                 database_type='A', \n",
    "                 \n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "        self.number_class, self.number_channel = numberClassChannel(database_type)\n",
    "        parameters.number_channel = self.number_channel\n",
    "        self.cnn = TCANet(\n",
    "                # Signal related parameters\n",
    "                n_chans=self.number_channel,\n",
    "                out_features=self.number_class,\n",
    "\n",
    "            ) \n",
    "    def forward(self, x):\n",
    "        out, features = self.cnn(x)\n",
    "        return features, out\n",
    "\n",
    "\n",
    "class ExP():\n",
    "    def __init__(self, nsub, data_dir, result_name,\n",
    "                 parameters,\n",
    "                 evaluate_mode = 'LOSO-no',\n",
    "                 dataset_type='A',\n",
    "                 n_fold = 0,\n",
    "                 ):\n",
    "        \n",
    "        super(ExP, self).__init__()\n",
    "        self.n_fold = n_fold\n",
    "        self.dataset_type = dataset_type\n",
    "        self.batch_size = parameters.batch_size\n",
    "        self.lr = parameters.learning_rate\n",
    "        self.b1 = 0.5\n",
    "        self.b2 = 0.999\n",
    "        self.n_epochs = parameters.epochs\n",
    "        self.nSub = nsub\n",
    "        self.nFold = n_fold\n",
    "        self.number_augmentation = parameters.number_aug\n",
    "        self.number_seg = parameters.number_seg\n",
    "        self.root = data_dir\n",
    "        self.result_name = result_name\n",
    "        self.evaluate_mode = evaluate_mode\n",
    "        self.Tensor = torch.cuda.FloatTensor\n",
    "        self.LongTensor = torch.cuda.LongTensor\n",
    "        self.criterion_cls = torch.nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "        self.number_class, self.number_channel = numberClassChannel(dataset_type)\n",
    "        self.model = EEGTransformer(\n",
    "            database_type=self.dataset_type, \n",
    "            parameters = parameters, \n",
    "            ).cuda()\n",
    "        #self.model = nn.DataParallel(self.model, device_ids=gpus)\n",
    "        self.model = self.model.cuda()\n",
    "        self.model_filename = self.result_name + '/model_nsub_{}_nfold_{}.pth'.format(self.nSub, n_fold+1)\n",
    "\n",
    "    def interaug(self, timg, label):  \n",
    "        aug_data = []\n",
    "        aug_label = []\n",
    "        number_records_by_augmentation = self.number_augmentation * int(self.batch_size / self.number_class)\n",
    "        number_segmentation_points = 1000 // self.number_seg\n",
    "        for clsAug in range(self.number_class):\n",
    "            cls_idx = np.where(label == clsAug + 1)\n",
    "            tmp_data = timg[cls_idx]\n",
    "            tmp_label = label[cls_idx]\n",
    "            tmp_aug_data = np.zeros((number_records_by_augmentation, 1, self.number_channel, 1000))\n",
    "            for ri in range(number_records_by_augmentation):\n",
    "                for rj in range(self.number_seg):\n",
    "                    rand_idx = np.random.randint(0, tmp_data.shape[0], self.number_seg)\n",
    "                    tmp_aug_data[ri, :, :, rj * number_segmentation_points:(rj + 1) * number_segmentation_points] = \\\n",
    "                        tmp_data[rand_idx[rj], :, :, rj * number_segmentation_points:(rj + 1) * number_segmentation_points]\n",
    "\n",
    "            aug_data.append(tmp_aug_data)\n",
    "            aug_label.append(tmp_label[:number_records_by_augmentation])\n",
    "        aug_data = np.concatenate(aug_data)\n",
    "        aug_label = np.concatenate(aug_label)\n",
    "        aug_shuffle = np.random.permutation(len(aug_data))\n",
    "        aug_data = aug_data[aug_shuffle, :, :]\n",
    "        aug_label = aug_label[aug_shuffle]\n",
    "\n",
    "        aug_data = torch.from_numpy(aug_data).cuda()\n",
    "        aug_data = aug_data.float()\n",
    "        aug_label = torch.from_numpy(aug_label-1).cuda()\n",
    "        aug_label = aug_label.long()\n",
    "        return aug_data, aug_label\n",
    "\n",
    "\n",
    "\n",
    "    def get_source_data(self):\n",
    "        (self.train_data,    # (batch, channel, length)\n",
    "         self.train_label, \n",
    "         self.test_data, \n",
    "         self.test_label) = load_data_evaluate(self.root, self.dataset_type, self.nSub, mode_evaluate=self.evaluate_mode)\n",
    "\n",
    "        self.train_data = np.expand_dims(self.train_data, axis=1)  # (288, 1, 22, 1000)\n",
    "        self.train_label = np.transpose(self.train_label)  \n",
    "\n",
    "        self.allData = self.train_data\n",
    "        self.allLabel = self.train_label[0]  \n",
    "\n",
    "        shuffle_num = np.random.permutation(len(self.allData))\n",
    "        self.allData = self.allData[shuffle_num, :, :, :]  # (288, 1, 22, 1000)\n",
    "        self.allLabel = self.allLabel[shuffle_num]\n",
    "\n",
    "        print('-'*20, \n",
    "              \"raw train size：\", self.train_data.shape, \n",
    "              \"test size：\", self.test_data.shape, \n",
    "              \"subject:\", self.nSub,\n",
    "              \"fold:\", self.nFold+1)\n",
    "        # self.test_data = np.transpose(self.test_data, (2, 1, 0))\n",
    "        self.test_data = np.expand_dims(self.test_data, axis=1)\n",
    "        self.test_label = np.transpose(self.test_label)\n",
    "\n",
    "        self.testData = self.test_data\n",
    "        self.testLabel = self.test_label[0]\n",
    "        \n",
    "        \n",
    "        \n",
    "        # standardize\n",
    "        target_mean = np.mean(self.allData)\n",
    "        target_std = np.std(self.allData)\n",
    "        self.allData = (self.allData - target_mean) / target_std\n",
    "        self.testData = (self.testData - target_mean) / target_std\n",
    "        \n",
    "        isSaveDataLabel = False #True\n",
    "        if isSaveDataLabel:\n",
    "            np.save(\"./gradm_data/train_data_{}.npy\".format(self.nSub), self.allData)\n",
    "            np.save(\"./gradm_data/train_lable_{}.npy\".format(self.nSub), self.allLabel)\n",
    "            np.save(\"./gradm_data/test_data_{}.npy\".format(self.nSub), self.testData)\n",
    "            np.save(\"./gradm_data/test_label_{}.npy\".format(self.nSub), self.testLabel)\n",
    "\n",
    "        \n",
    "        # data shape: (trial, conv channel, electrode channel, time samples)\n",
    "        return self.allData, self.allLabel, self.testData, self.testLabel\n",
    "\n",
    "    \n",
    "    def test_model(self, model, dataloader):\n",
    "        model.eval()\n",
    "        outputs_list = []\n",
    "        label_list = []\n",
    "        with torch.no_grad():\n",
    "            for i, (img, label) in enumerate(dataloader):\n",
    "                # val model\n",
    "                img = img.type(self.Tensor).cuda()\n",
    "                label = label.type(self.LongTensor).cuda()\n",
    "                _, Cls = model(img)\n",
    "                outputs_list.append(Cls)\n",
    "                del img, Cls\n",
    "                torch.cuda.empty_cache()\n",
    "                label_list.append(label)\n",
    "            \n",
    "        Cls = torch.cat(outputs_list)\n",
    "        val_label = torch.cat(label_list)\n",
    "        val_loss = self.criterion_cls(Cls, val_label)\n",
    "        val_pred = torch.max(Cls, 1)[1]\n",
    "        val_acc = float((val_pred == val_label).cpu().numpy().astype(int).sum()) / float(val_label.size(0))\n",
    "        return val_acc, val_loss, val_pred\n",
    "\n",
    "    \n",
    "\n",
    "    def train(self):\n",
    "        timg, label, test_data, test_label = self.get_source_data()\n",
    "        train_data_list_per_class = []\n",
    "        train_label_list_per_class = []\n",
    "        for clsAug in range(self.number_class):\n",
    "            cls_idx = np.where(label == clsAug + 1)\n",
    "            tmp_data = timg[cls_idx]\n",
    "            tmp_label = label[cls_idx]        \n",
    "            train_data_list_per_class.append(tmp_data)\n",
    "            train_label_list_per_class.append(tmp_label)\n",
    "            \n",
    "        train_data_list = []\n",
    "        train_label_list = []\n",
    "        val_data_list, val_label_list =  [], []\n",
    "        seed = 1234+self.nSub\n",
    "        for clsAug in range(self.number_class):\n",
    "            number_samples = len(train_data_list_per_class[clsAug])\n",
    "            number_test = number_samples // 5      # for validation\n",
    "            # Index is in random order, used to split the training set and test set\n",
    "            index_list = list(range(number_samples))\n",
    "            np.random.seed(seed+clsAug)\n",
    "            # Random index, fetching data based on the index, equivalent to shuffle the data set\n",
    "            index_shuffled = np.random.permutation(index_list)    \n",
    "            if self.n_fold!=4 :\n",
    "                index_val = [i for i in range(self.n_fold*number_test, (self.n_fold+1)*number_test)]\n",
    "            else:\n",
    "                # Since 288 (BCI IV-2a & IV-2b) cannot be divided by 5, the last fold is all the remaining\n",
    "                index_val = [i for i in range(self.n_fold*number_test, number_samples)]\n",
    "\n",
    "            index_train = [i for i in range(number_samples) if i not in index_val]\n",
    "            # Indexes of training and test sets\n",
    "            index_train = index_shuffled[index_train]\n",
    "            index_val = index_shuffled[index_val]   \n",
    "            \n",
    "            train_data_class = train_data_list_per_class[clsAug][index_train]\n",
    "            train_label_class = train_label_list_per_class[clsAug][index_train]\n",
    "            train_data_list.append(train_data_class)\n",
    "            train_label_list.append(train_label_class)\n",
    "            \n",
    "            val_data_class = train_data_list_per_class[clsAug][index_val]\n",
    "            val_label_class = train_label_list_per_class[clsAug][index_val]            \n",
    "            val_data_list.append(val_data_class)\n",
    "            val_label_list.append(val_label_class)        \n",
    "        img = np.concatenate(train_data_list)\n",
    "        label = np.concatenate(train_label_list)\n",
    "        val_data = np.concatenate(val_data_list)\n",
    "        val_label = np.concatenate(val_label_list)\n",
    "      \n",
    "        \n",
    "        print('-'*20, \n",
    "              \"train size：\", img.shape, \n",
    "              \"val size：\", val_data.shape, )\n",
    "        \n",
    "        img = torch.from_numpy(img)\n",
    "        label = torch.from_numpy(label - 1)\n",
    "        dataset = torch.utils.data.TensorDataset(img, label)\n",
    "        self.dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        \n",
    "        val_data = torch.from_numpy(val_data)\n",
    "        val_label = torch.from_numpy(val_label - 1)\n",
    "        val_dataset = torch.utils.data.TensorDataset(val_data, val_label)\n",
    "        self.val_dataloader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "                \n",
    "        \n",
    "        test_data = torch.from_numpy(test_data)\n",
    "        test_label = torch.from_numpy(test_label - 1)\n",
    "        test_dataset = torch.utils.data.TensorDataset(test_data, test_label)\n",
    "        self.test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        # Optimizers\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, betas=(self.b1, self.b2),)\n",
    "\n",
    "        test_data = Variable(test_data.type(self.Tensor))\n",
    "        test_label = Variable(test_label.type(self.LongTensor))\n",
    "        best_epoch = 0\n",
    "        num = 0\n",
    "        min_loss = 100\n",
    "        max_acc = 0\n",
    "        # recording train_acc, train_loss, test_acc, test_loss\n",
    "        result_process = []\n",
    "        # Train the cnn model\n",
    "        for e in range(self.n_epochs):\n",
    "            epoch_process = {}\n",
    "            epoch_process['epoch'] = e\n",
    "            # in_epoch = time.time()\n",
    "            self.model.train()\n",
    "            outputs_list = []\n",
    "            label_list = []\n",
    "            for i, (img, label) in enumerate(self.dataloader):\n",
    "                number_sample = img.shape[0]\n",
    "                \n",
    "                # split raw train dataset into real train dataset and validate dataset\n",
    "                train_data = img\n",
    "                train_label = label\n",
    "\n",
    "                \n",
    "                # real train dataset\n",
    "                img = Variable(train_data.type(self.Tensor))\n",
    "                label = Variable(train_label.type(self.LongTensor))\n",
    "                \n",
    "                # data augmentation\n",
    "                aug_data, aug_label = self.interaug(self.allData, self.allLabel)\n",
    "                # concat real train dataset and generate aritifical train dataset\n",
    "                img = torch.cat((img, aug_data))\n",
    "                label = torch.cat((label, aug_label))\n",
    "\n",
    "                # training model\n",
    "                features, outputs = self.model(img)\n",
    "                outputs_list.append(outputs)\n",
    "                label_list.append(label)\n",
    "                # print(\"train outputs: \", outputs.shape, type(outputs))\n",
    "                # print(features.size())\n",
    "                loss = self.criterion_cls(outputs, label) \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            del img\n",
    "            torch.cuda.empty_cache()\n",
    "            # out_epoch = time.time()\n",
    "            # test process\n",
    "            if (e + 1) % 1 == 0:\n",
    "                self.model.eval()\n",
    "                # validate model\n",
    "                val_acc, val_loss, _ = self.test_model(self.model, self.val_dataloader)\n",
    "                \n",
    "                epoch_process['val_acc'] = val_acc                \n",
    "                epoch_process['val_loss'] = val_loss.detach().cpu().numpy()  \n",
    "                \n",
    "                train_pred = torch.max(outputs, 1)[1]\n",
    "                train_acc = float((train_pred == label).cpu().numpy().astype(int).sum()) / float(label.size(0))\n",
    "                epoch_process['train_acc'] = train_acc\n",
    "                epoch_process['train_loss'] = loss.detach().cpu().numpy()\n",
    "\n",
    "                num = num + 1\n",
    "\n",
    "                # if min_loss>val_loss:                \n",
    "                if max_acc<val_acc or (max_acc==val_acc and min_loss>val_loss):\n",
    "                    max_acc = val_acc\n",
    "                    min_loss = val_loss\n",
    "                    \n",
    "                    best_epoch = e\n",
    "                    epoch_process['epoch'] = e\n",
    "                    torch.save(self.model, self.model_filename)\n",
    "                    test_acc, test_loss, y_pred  = self.test_model(self.model, self.test_dataloader)\n",
    "                    print(\"{}_{} train_acc: {:.4f} train_loss: {:.6f}\\tval_acc: {:.6f} val_loss: {:.7f} test_acc:{:.6f}\".format(self.nSub,\n",
    "                                                                                           epoch_process['epoch'],\n",
    "                                                                                           epoch_process['train_acc'],\n",
    "                                                                                           epoch_process['train_loss'],\n",
    "                                                                                           epoch_process['val_acc'],\n",
    "                                                                                           epoch_process['val_loss'],\n",
    "                                                                                           test_acc                                     \n",
    "                                                                                        ))\n",
    "            \n",
    "                \n",
    "            result_process.append(epoch_process)  \n",
    "\n",
    "        \n",
    "        # load model for test\n",
    "        self.model.eval()\n",
    "        self.model = torch.load(self.model_filename).cuda()\n",
    "        outputs_list = []\n",
    "        with torch.no_grad():\n",
    "            for i, (img, label) in enumerate(self.test_dataloader):\n",
    "                img_test = Variable(img.type(self.Tensor)).cuda()\n",
    "                # label_test = Variable(label.type(self.LongTensor))\n",
    "\n",
    "                # test model\n",
    "                features, outputs = self.model(img_test)\n",
    "                val_pred = torch.max(outputs, 1)[1]\n",
    "                outputs_list.append(outputs)\n",
    "        outputs = torch.cat(outputs_list) \n",
    "        y_pred = torch.max(outputs, 1)[1]\n",
    "        \n",
    "        \n",
    "        test_acc = float((y_pred == test_label).cpu().numpy().astype(int).sum()) / float(test_label.size(0))\n",
    "        \n",
    "        print(\"epoch: \", best_epoch, '\\tThe test accuracy is:', test_acc)\n",
    "\n",
    "\n",
    "        df_process = pd.DataFrame(result_process)\n",
    "\n",
    "        return test_acc, test_label, y_pred, df_process, best_epoch, outputs\n",
    "        # writer.close()\n",
    "        \n",
    "\n",
    "def main(dirs,           \n",
    "         paramters,\n",
    "         evaluate_mode = 'subject-dependent', \n",
    "         dataset_type='A',    # A->'BCI IV2a', B->'BCI IV2b'\n",
    "         ):\n",
    "\n",
    "    if not os.path.exists(dirs):\n",
    "        os.makedirs(dirs)\n",
    "    result_write_metric = ExcelWriter(dirs+\"/result_metric.xlsx\")\n",
    "    result_metric_dict = {}\n",
    "    y_true_pred_dict = { }\n",
    "\n",
    "    process_write = ExcelWriter(dirs+\"/process_train.xlsx\")\n",
    "    pred_true_write = ExcelWriter(dirs+\"/pred_true.xlsx\")\n",
    "    pred_softmax = ExcelWriter(dirs+\"/pred_softmax.xlsx\")\n",
    "    subjects_result = []\n",
    "    \n",
    "    \n",
    "    best_epochs = []\n",
    "    result_fold = []\n",
    "    for i in range(paramters.subject_number):      \n",
    "        starttime = datetime.datetime.now()\n",
    "        seed_n = np.random.randint(2024)\n",
    "        print('seed is ' + str(seed_n))\n",
    "        random.seed(seed_n)\n",
    "        np.random.seed(seed_n)\n",
    "        torch.manual_seed(seed_n)\n",
    "        torch.cuda.manual_seed(seed_n)\n",
    "        torch.cuda.manual_seed_all(seed_n)\n",
    "        index_round =0\n",
    "        print('Subject %d' % (i+1))\n",
    "\n",
    "        subjects_result_fold = []\n",
    "        # If you want to do cross validation, this should be set to 5. Currently, \n",
    "        for n_fold in range(1):     #  only 20% is used as the validation set, and only 1.\n",
    "            exp = ExP(i + 1, DATA_DIR, dirs, \n",
    "                      paramters,\n",
    "                      evaluate_mode = evaluate_mode,\n",
    "                      dataset_type=dataset_type,\n",
    "                      n_fold=n_fold,\n",
    "                      )\n",
    "\n",
    "            testAcc, Y_true, Y_pred, df_process, best_epoch,pred_output = exp.train()\n",
    "            probs = torch.softmax(pred_output, dim=1).cpu().numpy()\n",
    "            df_probs = pd.DataFrame(probs)\n",
    "            df_probs.to_excel(pred_softmax, sheet_name=str(i+1)+'_'+str(n_fold))\n",
    "            true_cpu = Y_true.cpu().numpy().astype(int)\n",
    "            pred_cpu = Y_pred.cpu().numpy().astype(int)\n",
    "            df_pred_true = pd.DataFrame({'pred': pred_cpu, 'true': true_cpu})\n",
    "            df_pred_true.to_excel(pred_true_write, sheet_name=str(i+1)+'_'+str(n_fold))\n",
    "            y_true_pred_dict[i] = df_pred_true\n",
    "\n",
    "            accuracy, precison, recall, f1, kappa = calMetrics(true_cpu, pred_cpu)\n",
    "            subject_result = {'accuray': accuracy*100,\n",
    "                              'precision': precison*100,\n",
    "                              'recall': recall*100,\n",
    "                              'f1': f1*100, \n",
    "                              'kappa': kappa*100\n",
    "                              }\n",
    "            df_process.to_excel(process_write, sheet_name=str(i+1)+'_'+str(n_fold))\n",
    "            best_epochs.append(best_epoch)\n",
    "            print(' THE BEST ACCURACY IS ' + str(testAcc) + \"\\tkappa is \" + str(kappa) )\n",
    "    \n",
    "\n",
    "            endtime = datetime.datetime.now()\n",
    "            print('subject %d duration: '%(i+1) + str(endtime - starttime))\n",
    "\n",
    "            if i == 0:\n",
    "                yt = Y_true\n",
    "                yp = Y_pred\n",
    "            else:\n",
    "                yt = torch.cat((yt, Y_true))\n",
    "                yp = torch.cat((yp, Y_pred))\n",
    "            subjects_result_fold.append(subject_result)\n",
    "            df_result_fold = pd.DataFrame(subjects_result)\n",
    "            \n",
    "        df = pd.DataFrame(subjects_result_fold)\n",
    "        df.to_excel(result_write_metric, index=False,  sheet_name=str(i+1))\n",
    "        result_fold_mean = df.mean()\n",
    "        print(\"{} subject {} fold mean: \\n {}\".format(i+1, n_fold+1, result_fold_mean))\n",
    "        subjects_result.append(result_fold_mean)\n",
    "    df_result = pd.DataFrame(subjects_result)\n",
    "    process_write.close()\n",
    "    pred_true_write.close()\n",
    "    pred_softmax.close()\n",
    "\n",
    "\n",
    "    print('**The average Best accuracy is: ' + str(df_result['accuray'].mean()) + \"kappa is: \" + str(df_result['kappa'].mean()) + \"\\n\" )\n",
    "    print(\"best epochs: \", best_epochs)\n",
    "    #df_result.to_excel(result_write_metric, index=False)\n",
    "    result_metric_dict = df_result\n",
    "\n",
    "    mean = df_result.mean(axis=0)\n",
    "    mean.name = 'mean'\n",
    "    std = df_result.std(axis=0)\n",
    "    std.name = 'std'\n",
    "    df_result = pd.concat([df_result, pd.DataFrame(mean).T, pd.DataFrame(std).T])\n",
    "    \n",
    "    df_result.to_excel(result_write_metric, index=False, sheet_name='mean')\n",
    "    print('-'*9, ' all result ', '-'*9)\n",
    "    print(df_result)\n",
    "    \n",
    "    print(\"*\"*40)\n",
    "\n",
    "    result_write_metric.close()\n",
    "\n",
    "    \n",
    "    return result_metric_dict\n",
    "\n",
    "\n",
    "class Parameters():\n",
    "    def __init__(self, dropout_rate):\n",
    "        self.heads = 8\n",
    "        self.depth = 5\n",
    "        self.emb_size = 16*3\n",
    "        self.f1 = 16\n",
    "        self.pooling_size = 52\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.subject_number = 9\n",
    "        self.learning_rate = 0.001\n",
    "        self.batch_size = 72 \n",
    "        self.epochs=1000\n",
    "        self.number_aug=1\n",
    "        # The actual number of training batches is: self.batch_size*(1+self.number_aug)\n",
    "        self.number_seg=8\n",
    "        self.gpus=gpus        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #----------------------------------------\n",
    "    DATA_DIR = r'../mymat_raw/'\n",
    "    EVALUATE_MODE = 'LOSO-No' # leaving one subject out subject-dependent  subject-indenpedent\n",
    "\n",
    "    TYPE = 'A'\n",
    "    if EVALUATE_MODE!='LOSO':\n",
    "        CNN_DROPOUT_RATE = 0.5\n",
    "    else:\n",
    "        CNN_DROPOUT_RATE = 0.25    \n",
    "    \n",
    "    parameters = Parameters(CNN_DROPOUT_RATE)\n",
    "    POOLING_SIZE = 56\n",
    "    HEADS = 2\n",
    "    DEPTH = 6\n",
    "\n",
    "    number_class, number_channel = numberClassChannel(TYPE)\n",
    "    RESULT_NAME = \"TCANet_{}\".format(TYPE)\n",
    "\n",
    "    sModel = EEGTransformer(\n",
    "        database_type=TYPE, \n",
    "        parameters = parameters,  \n",
    "        ).cuda()\n",
    "    summary(sModel, (1, number_channel, 1000)) \n",
    "\n",
    "    print(time.asctime(time.localtime(time.time())))\n",
    "\n",
    "    result = main(RESULT_NAME,\n",
    "                  parameters,\n",
    "                    evaluate_mode = EVALUATE_MODE,\n",
    "                    dataset_type=TYPE,\n",
    "                  )\n",
    "    print(time.asctime(time.localtime(time.time())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a348872e-f0dc-431d-8bdd-db4b534e9f42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
